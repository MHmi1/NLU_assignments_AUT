{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "meK0JpbEPAAA",
        "outputId": "7762859c-18c7-42c4-b22e-5bd940500da0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "execution_count": 1,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuJFBFbWOW7t",
        "outputId": "6d8f3092-d3e2-4c7a-e404-bf33d099c4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://amazon-massive-nlu-dataset.s3.amazonaws.com/amazon-massive-dataset-1.0.tar.gz\n",
            "To: /content/amazon-massive-dataset-1.0.tar.gz\n",
            "100% 39.5M/39.5M [00:04<00:00, 8.34MB/s]\n",
            "1.0/\n",
            "1.0/CITATION.md\n",
            "1.0/NOTICE.md\n",
            "1.0/data/\n",
            "1.0/data/mn-MN.jsonl\n",
            "1.0/data/af-ZA.jsonl\n",
            "1.0/data/el-GR.jsonl\n",
            "1.0/data/ta-IN.jsonl\n",
            "1.0/data/ar-SA.jsonl\n",
            "1.0/data/ur-PK.jsonl\n",
            "1.0/data/pl-PL.jsonl\n",
            "1.0/data/ko-KR.jsonl\n",
            "1.0/data/az-AZ.jsonl\n",
            "1.0/data/da-DK.jsonl\n",
            "1.0/data/kn-IN.jsonl\n",
            "1.0/data/tl-PH.jsonl\n",
            "1.0/data/is-IS.jsonl\n",
            "1.0/data/lv-LV.jsonl\n",
            "1.0/data/it-IT.jsonl\n",
            "1.0/data/es-ES.jsonl\n",
            "1.0/data/fr-FR.jsonl\n",
            "1.0/data/ml-IN.jsonl\n",
            "1.0/data/km-KH.jsonl\n",
            "1.0/data/fa-IR.jsonl\n",
            "1.0/data/sw-KE.jsonl\n",
            "1.0/data/en-US.jsonl\n",
            "1.0/data/tr-TR.jsonl\n",
            "1.0/data/bn-BD.jsonl\n",
            "1.0/data/he-IL.jsonl\n",
            "1.0/data/te-IN.jsonl\n",
            "1.0/data/pt-PT.jsonl\n",
            "1.0/data/ka-GE.jsonl\n",
            "1.0/data/ja-JP.jsonl\n",
            "1.0/data/id-ID.jsonl\n",
            "1.0/data/ru-RU.jsonl\n",
            "1.0/data/hy-AM.jsonl\n",
            "1.0/data/nb-NO.jsonl\n",
            "1.0/data/ms-MY.jsonl\n",
            "1.0/data/sq-AL.jsonl\n",
            "1.0/data/sv-SE.jsonl\n",
            "1.0/data/fi-FI.jsonl\n",
            "1.0/data/th-TH.jsonl\n",
            "1.0/data/de-DE.jsonl\n",
            "1.0/data/vi-VN.jsonl\n",
            "1.0/data/my-MM.jsonl\n",
            "1.0/data/jv-ID.jsonl\n",
            "1.0/data/sl-SL.jsonl\n",
            "1.0/data/hi-IN.jsonl\n",
            "1.0/data/nl-NL.jsonl\n",
            "1.0/data/zh-CN.jsonl\n",
            "1.0/data/hu-HU.jsonl\n",
            "1.0/data/ro-RO.jsonl\n",
            "1.0/data/zh-TW.jsonl\n",
            "1.0/data/cy-GB.jsonl\n",
            "1.0/data/am-ET.jsonl\n",
            "1.0/LICENSE\n"
          ]
        }
      ],
      "source": [
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "!gdown https://amazon-massive-nlu-dataset.s3.amazonaws.com/amazon-massive-dataset-1.0.tar.gz\n",
        "!tar -xvf /content/amazon-massive-dataset-1.0.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVAPuYh4t8Pp",
        "outputId": "da8990a3-a484-453f-a19c-4c12a08de69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing train folder...\n",
            "Skipping record due to mismatch: ['همه', 'کارهای', 'مهاجرتم', 'رو', 'انجام', 'دادم', 'مونده', 'فقط', 'گرفتن', 'ارز.', 'به', '3200', 'دلار', 'واسه', 'کشور', 'استرالیا', 'نیاز', 'دارم.', 'برام', 'ردیفش', 'کن', 'لطفاً'] vs ['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'b-amount', 'b-currency', 'o', 'b-country', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "Processing validation folder...\n",
            "Creating intent.txt and slot.txt...\n",
            "Skipping record in vocab creation due to mismatch: همه کارهای مهاجرتم رو انجام دادم مونده فقط گرفتن ارز. به 3200 دلار واسه کشور استرالیا نیاز دارم. برام ردیفش کن لطفاً vs ['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'b-amount', 'b-currency', 'o', 'b-country', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "Files created: train.txt, validation.txt, intent.txt, slot.txt\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def parse_line(item, intent_map):\n",
        "    \"\"\"\n",
        "    Parse a single line of data to create token-label pairs and map intent IDs to intent names.\n",
        "    \"\"\"\n",
        "    utt = item['input_text']\n",
        "    slots = item['slots']\n",
        "    intent = intent_map[item['intent_id']]  # Map intent_id to intent name\n",
        "\n",
        "    tokens = utt.split()\n",
        "    if len(tokens) != len(slots):\n",
        "        # Log the problematic record and skip it\n",
        "        print(f\"Skipping record due to mismatch: {tokens} vs {slots}\")\n",
        "        return None, None\n",
        "\n",
        "    # Create token-label pairs\n",
        "    labeled_tokens = [f\"{token}:{slot}\" for token, slot in zip(tokens, slots)]\n",
        "    return ' '.join(labeled_tokens), intent\n",
        "\n",
        "\n",
        "def read_json_files(data_folder):\n",
        "    \"\"\"\n",
        "    Read all JSON files in the folder and return a list of dictionaries.\n",
        "    Supports both JSON and JSON Lines formats.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    for file_name in os.listdir(data_folder):\n",
        "        if file_name.endswith(\".json\"):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    # Try loading as a JSON file\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, list):  # If it's a list of dictionaries\n",
        "                        all_data.extend(data)\n",
        "                    else:  # If it's a single dictionary\n",
        "                        all_data.append(data)\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON decoding fails, assume it's JSON Lines format\n",
        "                    f.seek(0)  # Reset file pointer\n",
        "                    for line in f:\n",
        "                        all_data.append(json.loads(line.strip()))\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def create_txt_file(data_folder, intent_map, output_file):\n",
        "    \"\"\"\n",
        "    Create a .txt file (train.txt or validation.txt) for a given dataset folder.\n",
        "    \"\"\"\n",
        "    all_data = read_json_files(data_folder)  # Read all JSON data from the folder\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for item in all_data:\n",
        "            parsed_line, intent = parse_line(item, intent_map)\n",
        "            if parsed_line and intent:  # Skip invalid records\n",
        "                f.write(f\"{parsed_line} <=> {intent}\\n\")\n",
        "\n",
        "\n",
        "def create_vocab_files(train_folder, validation_folder, intent_map, intent_file, slot_file):\n",
        "    \"\"\"\n",
        "    Create intent.txt and slot.txt files based on data from both train and validation folders.\n",
        "    \"\"\"\n",
        "    intents = set()\n",
        "    slots = set()\n",
        "\n",
        "    # Process both train and validation folders\n",
        "    for data_folder in [train_folder, validation_folder]:\n",
        "        all_data = read_json_files(data_folder)\n",
        "        for item in all_data:\n",
        "            if len(item['input_text'].split()) != len(item['slots']):\n",
        "                # Skip invalid records\n",
        "                print(f\"Skipping record in vocab creation due to mismatch: {item['input_text']} vs {item['slots']}\")\n",
        "                continue\n",
        "\n",
        "            intents.add(intent_map[item['intent_id']])\n",
        "            slots.update(item['slots'])\n",
        "\n",
        "    # Add 'O' to the slot vocabulary\n",
        "    slots.add('O')\n",
        "\n",
        "    # Write intent.txt\n",
        "    with open(intent_file, 'w', encoding='utf-8') as f:\n",
        "        for intent in sorted(intents):\n",
        "            f.write(f\"{intent}\\n\")\n",
        "\n",
        "    # Write slot.txt\n",
        "    with open(slot_file, 'w', encoding='utf-8') as f:\n",
        "        for slot in sorted(slots):\n",
        "            f.write(f\"{slot}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    train_folder = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/train\"\n",
        "    validation_folder = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/validation\"\n",
        "\n",
        "    # Output files\n",
        "    train_output = \"train.txt\"\n",
        "    validation_output = \"validation.txt\"\n",
        "    intent_output = \"intent.txt\"\n",
        "    slot_output = \"slot.txt\"\n",
        "\n",
        "    # Intent mapping (based on the list you provided)\n",
        "    intent_map = {\n",
        "        10: \"open_account_free\",\n",
        "        11: \"open_account_current\",\n",
        "        12: \"open_account_deposit\",\n",
        "        20: \"loan_free\",\n",
        "        21: \"loan_interest\",\n",
        "        30: \"card2card\",\n",
        "        31: \"paya\",\n",
        "        32: \"convert_cheque\",\n",
        "        40: \"receipt_payment\",\n",
        "        41: \"installment_payment\",\n",
        "        50: \"turnover_bill\",\n",
        "        51: \"balance_bill\",\n",
        "        60: \"submit_cheque\",\n",
        "        61: \"recieve_cheque\",\n",
        "        70: \"change_password\",\n",
        "        71: \"duplicate_card\",\n",
        "        72: \"close_card\",\n",
        "        80: \"delegate_account\",\n",
        "        81: \"currency_request\",\n",
        "        90: \"software_problem\",\n",
        "        91: \"signin_problem\",\n",
        "    }\n",
        "\n",
        "    # Process the train folder\n",
        "    print(\"Processing train folder...\")\n",
        "    create_txt_file(train_folder, intent_map, train_output)\n",
        "\n",
        "    # Process the validation folder\n",
        "    print(\"Processing validation folder...\")\n",
        "    create_txt_file(validation_folder, intent_map, validation_output)\n",
        "\n",
        "    # Create intent.txt and slot.txt\n",
        "    print(\"Creating intent.txt and slot.txt...\")\n",
        "    create_vocab_files(train_folder, validation_folder, intent_map, intent_output, slot_output)\n",
        "\n",
        "    print(\"Files created: train.txt, validation.txt, intent.txt, slot.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## edit based on order ofd files "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing train folder...\n",
            "Skipping record due to mismatch: ['همه', 'کارهای', 'مهاجرتم', 'رو', 'انجام', 'دادم', 'مونده', 'فقط', 'گرفتن', 'ارز.', 'به', '3200', 'دلار', 'واسه', 'کشور', 'استرالیا', 'نیاز', 'دارم.', 'برام', 'ردیفش', 'کن', 'لطفاً'] vs ['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'b-amount', 'b-currency', 'o', 'b-country', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "Processing validation folder...\n",
            "Creating intent.txt and slot.txt...\n",
            "Skipping record in vocab creation due to mismatch: همه کارهای مهاجرتم رو انجام دادم مونده فقط گرفتن ارز. به 3200 دلار واسه کشور استرالیا نیاز دارم. برام ردیفش کن لطفاً vs ['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'b-amount', 'b-currency', 'o', 'b-country', 'o', 'o', 'o', 'o', 'o', 'o']\n",
            "Files created: train.txt, validation.txt, intent.txt, slot.txt\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def parse_line(item, intent_map):\n",
        "    \"\"\"\n",
        "    Parse a single line of data to create token-label pairs and map intent IDs to intent names.\n",
        "    \"\"\"\n",
        "    utt = item['input_text']\n",
        "    slots = item['slots']\n",
        "    intent = intent_map[item['intent_id']]  # Map intent_id to intent name\n",
        "\n",
        "    tokens = utt.split()\n",
        "    if len(tokens) != len(slots):\n",
        "        # Log the problematic record and skip it\n",
        "        print(f\"Skipping record due to mismatch: {tokens} vs {slots}\")\n",
        "        return None, None\n",
        "\n",
        "    # Create token-label pairs\n",
        "    labeled_tokens = [f\"{token}:{slot}\" for token, slot in zip(tokens, slots)]\n",
        "    return ' '.join(labeled_tokens), intent\n",
        "\n",
        "\n",
        "def read_json_files(data_folder):\n",
        "    \"\"\"\n",
        "    Read all JSON files in the folder (sorted alphabetically) and return a list of dictionaries.\n",
        "    Supports both JSON and JSON Lines formats.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    # Sort files alphabetically\n",
        "    sorted_files = sorted(os.listdir(data_folder))\n",
        "    for file_name in sorted_files:\n",
        "        if file_name.endswith(\".json\"):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    # Try loading as a JSON file\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, list):  # If it's a list of dictionaries\n",
        "                        all_data.extend(data)\n",
        "                    else:  # If it's a single dictionary\n",
        "                        all_data.append(data)\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON decoding fails, assume it's JSON Lines format\n",
        "                    f.seek(0)  # Reset file pointer\n",
        "                    for line in f:\n",
        "                        all_data.append(json.loads(line.strip()))\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def create_txt_file(data_folder, intent_map, output_file):\n",
        "    \"\"\"\n",
        "    Create a .txt file (train.txt or validation.txt) for a given dataset folder.\n",
        "    \"\"\"\n",
        "    all_data = read_json_files(data_folder)  # Read all JSON data from the folder\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for item in all_data:\n",
        "            parsed_line, intent = parse_line(item, intent_map)\n",
        "            if parsed_line and intent:  # Skip invalid records\n",
        "                f.write(f\"{parsed_line} <=> {intent}\\n\")\n",
        "\n",
        "\n",
        "def create_vocab_files(train_folder, validation_folder, intent_map, intent_file, slot_file):\n",
        "    \"\"\"\n",
        "    Create intent.txt and slot.txt files based on data from both train and validation folders.\n",
        "    \"\"\"\n",
        "    intents = set()\n",
        "    slots = set()\n",
        "\n",
        "    # Process both train and validation folders\n",
        "    for data_folder in [train_folder, validation_folder]:\n",
        "        all_data = read_json_files(data_folder)\n",
        "        for item in all_data:\n",
        "            if len(item['input_text'].split()) != len(item['slots']):\n",
        "                # Skip invalid records\n",
        "                print(f\"Skipping record in vocab creation due to mismatch: {item['input_text']} vs {item['slots']}\")\n",
        "                continue\n",
        "\n",
        "            intents.add(intent_map[item['intent_id']])\n",
        "            slots.update(item['slots'])\n",
        "\n",
        "    # Add 'O' to the slot vocabulary\n",
        "    slots.add('O')\n",
        "\n",
        "    # Write intent.txt\n",
        "    with open(intent_file, 'w', encoding='utf-8') as f:\n",
        "        for intent in sorted(intents):\n",
        "            f.write(f\"{intent}\\n\")\n",
        "\n",
        "    # Write slot.txt\n",
        "    with open(slot_file, 'w', encoding='utf-8') as f:\n",
        "        for slot in sorted(slots):\n",
        "            f.write(f\"{slot}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    train_folder = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/train\"\n",
        "    validation_folder = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/validation\"\n",
        "\n",
        "    # Output files\n",
        "    train_output = \"train.txt\"\n",
        "    validation_output = \"validation.txt\"\n",
        "    intent_output = \"intent.txt\"\n",
        "    slot_output = \"slot.txt\"\n",
        "\n",
        "    # Intent mapping (based on the list you provided)\n",
        "    intent_map = {\n",
        "        10: \"open_account_free\",\n",
        "        11: \"open_account_current\",\n",
        "        12: \"open_account_deposit\",\n",
        "        20: \"loan_free\",\n",
        "        21: \"loan_interest\",\n",
        "        30: \"card2card\",\n",
        "        31: \"paya\",\n",
        "        32: \"convert_cheque\",\n",
        "        40: \"receipt_payment\",\n",
        "        41: \"installment_payment\",\n",
        "        50: \"turnover_bill\",\n",
        "        51: \"balance_bill\",\n",
        "        60: \"submit_cheque\",\n",
        "        61: \"recieve_cheque\",\n",
        "        70: \"change_password\",\n",
        "        71: \"duplicate_card\",\n",
        "        72: \"close_card\",\n",
        "        80: \"delegate_account\",\n",
        "        81: \"currency_request\",\n",
        "        90: \"software_problem\",\n",
        "        91: \"signin_problem\",\n",
        "    }\n",
        "\n",
        "    # Process the train folder\n",
        "    print(\"Processing train folder...\")\n",
        "    create_txt_file(train_folder, intent_map, train_output)\n",
        "\n",
        "    # Process the validation folder\n",
        "    print(\"Processing validation folder...\")\n",
        "    create_txt_file(validation_folder, intent_map, validation_output)\n",
        "\n",
        "    # Create intent.txt and slot.txt\n",
        "    print(\"Creating intent.txt and slot.txt...\")\n",
        "    create_vocab_files(train_folder, validation_folder, intent_map, intent_output, slot_output)\n",
        "\n",
        "    print(\"Files created: train.txt, validation.txt, intent.txt, slot.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## normazlizing dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing digits in .txt files...\n",
            "Normalized digits in train.txt (in-place).\n",
            "Normalized digits in validation.txt (in-place).\n"
          ]
        }
      ],
      "source": [
        "def normalize_digits(text, to_persian=False):\n",
        "    \"\"\"\n",
        "    Normalize digits in the given text.\n",
        "    Converts Persian digits to English digits by default.\n",
        "    Set `to_persian=True` to convert English digits to Persian digits.\n",
        "    \"\"\"\n",
        "    if to_persian:\n",
        "        # Convert English digits to Persian digits\n",
        "        english_to_persian = str.maketrans(\"0123456789\", \"۰۱۲۳۴۵۶۷۸۹\")\n",
        "        return text.translate(english_to_persian)\n",
        "    else:\n",
        "        # Convert Persian digits to English digits\n",
        "        persian_to_english = str.maketrans(\"۰۱۲۳۴۵۶۷۸۹\", \"0123456789\")\n",
        "        return text.translate(persian_to_english)\n",
        "\n",
        "\n",
        "def normalize_txt_file_inplace(file_path, to_persian=False):\n",
        "    \"\"\"\n",
        "    Normalize digits in a .txt file (in-place).\n",
        "    Converts Persian digits to English digits by default.\n",
        "    \"\"\"\n",
        "    # Read the content of the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Normalize digits in each line\n",
        "    normalized_lines = [normalize_digits(line, to_persian) for line in lines]\n",
        "\n",
        "    # Write the normalized lines back to the same file (in-place)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        f.writelines(normalized_lines)\n",
        "\n",
        "    print(f\"Normalized digits in {file_path} (in-place).\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths to the .txt files\n",
        "    train_file = \"train.txt\"\n",
        "    validation_file = \"validation.txt\"\n",
        "\n",
        "    # Normalize digits in the .txt files (convert Persian digits to English)\n",
        "    print(\"Normalizing digits in .txt files...\")\n",
        "    normalize_txt_file_inplace(train_file, to_persian=False)\n",
        "    normalize_txt_file_inplace(validation_file, to_persian=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
