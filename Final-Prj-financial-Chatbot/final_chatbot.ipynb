{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Data from Train Folder (1 sample(s) per intent):\n",
      "\n",
      "Intent 41 (installment_payment):\n",
      "{\n",
      "    \"input_text\": \"وام 8083988 پلکانیه. این ماه قسطش 75000000 اس که باید پرداخت کنم. قسط 17 اشه\",\n",
      "    \"intent_id\": 41,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"b-loan_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-installment_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-installment_n\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 10 (open_account_free):\n",
      "{\n",
      "    \"input_text\": \"میخام حساب قرض‌الحسنه باز کنی. حساب بنام دریا حمیدی باشه. کدملیش هم 4967930814 تاریخ 11 فروردین 34 هم بدنیا اومده. اسم پدرش محمود\",\n",
      "    \"intent_id\": 10,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-fname\",\n",
      "        \"b-lname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-national_id\",\n",
      "        \"o\",\n",
      "        \"b-birth_date\",\n",
      "        \"i-birth_date\",\n",
      "        \"i-birth_date\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-father_name\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 70 (change_password):\n",
      "{\n",
      "    \"input_text\": \"لازمه که رمز کارتمو تغییر بدم رمز قبلیش 54321 بود الان میخوام بشه ۲۱۴۴۰ شمارش هم 6274129712948395 هست\",\n",
      "    \"intent_id\": 70,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-current_pass\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-new_pass\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-card_number\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 71 (duplicate_card):\n",
      "{\n",
      "    \"input_text\": \"کارت جدید برام صادر کن. شماره کارتم 6274123075600193 اس بنام محمد نجفی\",\n",
      "    \"intent_id\": 71,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-card_number\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-fname\",\n",
      "        \"b-lname\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 61 (recieve_cheque):\n",
      "{\n",
      "    \"input_text\": \"چکی که آن را دریافت کرده‌ام شناسه‌اش 3709704 و مبلغش ۱۵۰۰۰۰۰ ریال است. این چک در وجه آتنا علیپور است.\",\n",
      "    \"intent_id\": 61,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-sayad_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cheque_amount\",\n",
      "        \"i-cheque_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cfname\",\n",
      "        \"b-clname\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 12 (open_account_deposit):\n",
      "{\n",
      "    \"input_text\": \"با سلام من اومدم یک حساب سپرده با سود شانزده درصد افتتاح کنم اسمم محمد طاها فامیلیم دهقان تاریخ تولدم 5 خرداد 68 کدملیم ۲۲۹۷۶۶۲۴۷۶ هست\",\n",
      "    \"intent_id\": 12,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-benefit_rate\",\n",
      "        \"i-benefit_rate\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-fname\",\n",
      "        \"i-fname\",\n",
      "        \"o\",\n",
      "        \"b-lname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-birth_date\",\n",
      "        \"i-birth_date\",\n",
      "        \"i-birth_date\",\n",
      "        \"o\",\n",
      "        \"b-national_id\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 81 (currency_request):\n",
      "{\n",
      "    \"input_text\": \"دارم به همراه همسرم میرم سفر ترکیه بمقدار 3600 لیر نیازدارم برام اوکیش کن\",\n",
      "    \"intent_id\": 81,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-country\",\n",
      "        \"o\",\n",
      "        \"b-amount\",\n",
      "        \"b-currency\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 40 (receipt_payment):\n",
      "{\n",
      "    \"input_text\": \"تا 10 صبح ۲۰ بهمن وقت دارم که قبضم به شناسه 78201 و شناسه پرداخت 7145898 رو پرداختش کنم\",\n",
      "    \"intent_id\": 40,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"b-transfer_datetime\",\n",
      "        \"i-transfer_datetime\",\n",
      "        \"i-transfer_datetime\",\n",
      "        \"i-transfer_datetime\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-bill_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-payment_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 30 (card2card):\n",
      "{\n",
      "    \"input_text\": \"گوشیم هنگ کرده نمیتونم از طریق اپلیکیشن کارت به کارت کنم میخوام از کارت 6219-8670-0009-0999 مبلغ 40000000 رو همین الان به کارت 5892108781761200 بنام النا طاهری انتقال بدی رمز پویای کارتم ۱۲۳۴۸ رمز دوم کارت هم ۳۲۱\",\n",
      "    \"intent_id\": 30,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-card_number\",\n",
      "        \"o\",\n",
      "        \"b-transfer_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-receiver_card\",\n",
      "        \"o\",\n",
      "        \"b-fname\",\n",
      "        \"b-lname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-trans_pass\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cvv2\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 90 (software_problem):\n",
      "{\n",
      "    \"input_text\": \"برنامه بانکیم بنظر درست کار نمیکنه منو همش از حسابم خارج میکنه. چه مشکلی بنظرت پیش اومده؟\",\n",
      "    \"intent_id\": 90,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 91 (signin_problem):\n",
      "{\n",
      "    \"input_text\": \"وقتی اپلیکیشن موبایل بانکو باز میکنم یه صفحه سفید میاد بقیه برنامه ران نمیشه\",\n",
      "    \"intent_id\": 91,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 60 (submit_cheque):\n",
      "{\n",
      "    \"input_text\": \"من تا حالا چک صیادی ثبت نکردم و بلد نیستم باید چیکار کنم. تو می‌تونی کمکم کنی. یک چک ثبت کن به مبلغ 45000000 به نام همکارم اَنوشا لطفی به کد ملی ۷۰۱۳۳۵۵۳۳۳ منم ازت یاد می گیرم برای دفعات بعدی.\",\n",
      "    \"intent_id\": 60,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cheque_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cfname\",\n",
      "        \"b-clname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cnational_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 20 (loan_free):\n",
      "{\n",
      "    \"input_text\": \"توانایی من در بازپرداخت وام حدودا شانزده سال و نیم است. وام رو برای هزینه‌های بیمه میخام و مبلغش ۱۰۰ هزار ریال باشه برام خیلی خوب میشه. ضامنم هم علبرضا حیدری است با کدملی 1910516866\",\n",
      "    \"intent_id\": 20,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-loan_duration\",\n",
      "        \"i-loan_duration\",\n",
      "        \"i-loan_duration\",\n",
      "        \"i-loan_duration\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-loan_reason\",\n",
      "        \"i-loan_reason\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-loan_amount\",\n",
      "        \"i-loan_amount\",\n",
      "        \"i-loan_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-zfname\",\n",
      "        \"b-zlname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-znational_id\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 11 (open_account_current):\n",
      "{\n",
      "    \"input_text\": \"با مبلغ ۹۰ میلیون برای محمدرضا اسکندری با کد ملی ۲۱۸۳۹۱۷۳۴۵ نام پدر یوسف یک حساب جاری باز کنید\",\n",
      "    \"intent_id\": 11,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-starter_amount\",\n",
      "        \"i-starter_amount\",\n",
      "        \"o\",\n",
      "        \"b-fname\",\n",
      "        \"b-lname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-national_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-father_name\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 31 (paya):\n",
      "{\n",
      "    \"input_text\": \"با این سقف تراکنشها هر انتقال وجهی رو باید پایا کنی فقط. از حساب 5678901234 ام ۱۵۰۰۰۰۰ ریال رو برای شماره شبای IR760570010610010500089301 پایا کن\",\n",
      "    \"intent_id\": 31,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-account_id\",\n",
      "        \"o\",\n",
      "        \"b-transfer_amount\",\n",
      "        \"i-transfer_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-receiver_iban\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 72 (close_card):\n",
      "{\n",
      "    \"input_text\": \"به دلیل درخواست سازمان مالی میخوام کارت 5029104175029106 رو مسدودش کنی به اسم سارا اسکندری\",\n",
      "    \"intent_id\": 72,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-blocking_reason\",\n",
      "        \"i-blocking_reason\",\n",
      "        \"i-blocking_reason\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-card_number\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-fname\",\n",
      "        \"b-lname\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 21 (loan_interest):\n",
      "{\n",
      "    \"input_text\": \"با مبلغ ۹۰ میلیون درخواست وام با بهره بانکی پانزده درصد دارم برای اینکه برگردونم مهلت پرداختش تا سی سال باشه خیلی خوبه ضامنم مینو غفاری هست\",\n",
      "    \"intent_id\": 21,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-loan_amount\",\n",
      "        \"i-loan_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-loan_benefit_rate\",\n",
      "        \"i-loan_benefit_rate\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-loan_duration\",\n",
      "        \"i-loan_duration\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-zfname\",\n",
      "        \"b-zlname\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 50 (turnover_bill):\n",
      "{\n",
      "    \"input_text\": \"تو هفته پیش چندتا واریزو برداشت داشتم. وقت نکردم چک کنم ببینم اومده به حسابم یا نه گردش حساب هفته گذشته حساب 204-2-90123456-9 رو برام بفرست\",\n",
      "    \"intent_id\": 50,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-account_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 51 (balance_bill):\n",
      "{\n",
      "    \"input_text\": \"شماره حسابم 218000000001 اس. یه اعلام موجودی ازش بهم بده برای ۱۴۰۰/۱۰/۲۰\",\n",
      "    \"intent_id\": 51,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-account_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-balance_datetime\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 80 (delegate_account):\n",
      "{\n",
      "    \"input_text\": \"به دلیل امور بانکی میخوام حسابم از 204-4-12345678-1 بنام وکیلم شرکت مخابرات ایران بشه مدت زمانشم بعدا میگم\",\n",
      "    \"intent_id\": 80,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-advocacy_reason\",\n",
      "        \"i-advocacy_reason\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-account_id\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-name\",\n",
      "        \"i-name\",\n",
      "        \"i-name\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Intent 32 (convert_cheque):\n",
      "{\n",
      "    \"input_text\": \"واسه داداشم میخوام یه چک صادرکنم بنام ماهان فتحی با کدملی ۳۷۵۶۵۰۶۶۸۸ بابت پرداخت هزینه بیمه مبلغ چک 70 میلیون تومن و تاریخش 25 اسفند امسال\",\n",
      "    \"intent_id\": 32,\n",
      "    \"slots\": [\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cfname\",\n",
      "        \"b-clname\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cnational_id\",\n",
      "        \"o\",\n",
      "        \"b-transfer_reason\",\n",
      "        \"i-transfer_reason\",\n",
      "        \"i-transfer_reason\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-transfer_amount\",\n",
      "        \"i-transfer_amount\",\n",
      "        \"i-transfer_amount\",\n",
      "        \"o\",\n",
      "        \"o\",\n",
      "        \"b-cheque_date\",\n",
      "        \"i-cheque_date\",\n",
      "        \"i-cheque_date\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to the train folder\n",
    "train_folder = \"/home/mh/Downloads/Data-part1&2-v3/train\"\n",
    "\n",
    "# Intent mapping based on the provided data\n",
    "intent_mapping = {\n",
    "    10: \"open_account_free\",\n",
    "    11: \"open_account_current\",\n",
    "    12: \"open_account_deposit\",\n",
    "    20: \"loan_free\",\n",
    "    21: \"loan_interest\",\n",
    "    30: \"card2card\",\n",
    "    31: \"paya\",\n",
    "    32: \"convert_cheque\",\n",
    "    40: \"receipt_payment\",\n",
    "    41: \"installment_payment\",\n",
    "    50: \"turnover_bill\",\n",
    "    51: \"balance_bill\",\n",
    "    60: \"submit_cheque\",\n",
    "    61: \"recieve_cheque\",\n",
    "    70: \"change_password\",\n",
    "    71: \"duplicate_card\",\n",
    "    72: \"close_card\",\n",
    "    80: \"delegate_account\",\n",
    "    81: \"currency_request\",\n",
    "    90: \"software_problem\",\n",
    "    91: \"signin_problem\"\n",
    "}\n",
    "\n",
    "# Function to group files by intent\n",
    "def group_files_by_intent(folder_path):\n",
    "    \"\"\"\n",
    "    Group JSON files in the folder by their intent_id.\n",
    "    \"\"\"\n",
    "    grouped_data = defaultdict(list)\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                intent_id = data.get(\"intent_id\")\n",
    "                if intent_id is not None:\n",
    "                    grouped_data[intent_id].append(data)\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "# Function to sample data from each intent\n",
    "def sample_from_intents(grouped_data, num_samples=1):\n",
    "    \"\"\"\n",
    "    Sample a specified number of examples from each intent.\n",
    "    \"\"\"\n",
    "    sampled_data = {}\n",
    "\n",
    "    for intent_id, examples in grouped_data.items():\n",
    "        sampled_data[intent_id] = random.sample(examples, min(num_samples, len(examples)))\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Group files by intent\n",
    "    grouped_data = group_files_by_intent(train_folder)\n",
    "\n",
    "    # Sample data from each intent (e.g., 1 sample per intent)\n",
    "    num_samples_per_intent = 1  # Change this to sample more examples per intent\n",
    "    sampled_data = sample_from_intents(grouped_data, num_samples=num_samples_per_intent)\n",
    "\n",
    "    # Print the sampled data\n",
    "    print(f\"Sampled Data from Train Folder ({num_samples_per_intent} sample(s) per intent):\\n\")\n",
    "    for intent_id, samples in sampled_data.items():\n",
    "        print(f\"Intent {intent_id} ({intent_mapping.get(intent_id, 'Unknown')}):\")\n",
    "        for sample in samples:\n",
    "            print(json.dumps(sample, ensure_ascii=False, indent=4))\n",
    "        print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_31167/3108691314.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 291 tensors from /home/mh/Desktop/AVA-Llama-3-V2.i1-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = AVA Llama 3\n",
      "llama_model_loader: - kv   3:                            general.version str              = 3\n",
      "llama_model_loader: - kv   4:                       general.organization str              = MehdiHosseiniMoghadam\n",
      "llama_model_loader: - kv   5:                           general.basename str              = AVA-Llama\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128257\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128257\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  28:                                general.url str              = https://huggingface.co/mradermacher/A...\n",
      "llama_model_loader: - kv  29:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  30:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  31:                  mradermacher.quantized_at str              = 2025-01-03T18:54:42+01:00\n",
      "llama_model_loader: - kv  32:                  mradermacher.quantized_on str              = rain\n",
      "llama_model_loader: - kv  33:                         general.source.url str              = https://huggingface.co/MehdiHosseiniM...\n",
      "llama_model_loader: - kv  34:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = AVA-Llama-3-V2-i1-GGUF/imatrix.dat\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = imatrix-training-full-3\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 314\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128256 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 258\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128258\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 6.14 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = AVA Llama 3\n",
      "llm_load_print_meta: BOS token        = 128256 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOG token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q6_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  6282.98 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': 'imatrix-training-full-3', 'mradermacher.quantized_on': 'rain', 'quantize.imatrix.entries_count': '224', 'llama.attention.head_count_kv': '8', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/MehdiHosseiniMoghadam/AVA-Llama-3-V2', 'llama.feed_forward_length': '14336', 'general.size_label': '8.0B', 'general.type': 'model', 'general.organization': 'MehdiHosseiniMoghadam', 'general.version': '3', 'quantize.imatrix.file': 'AVA-Llama-3-V2-i1-GGUF/imatrix.dat', 'llama.rope.dimension_count': '128', 'quantize.imatrix.chunks_count': '314', 'llama.context_length': '8192', 'llama.embedding_length': '4096', 'mradermacher.quantized_at': '2025-01-03T18:54:42+01:00', 'llama.block_count': '32', 'llama.attention.head_count': '32', 'general.name': 'AVA Llama 3', 'tokenizer.ggml.bos_token_id': '128256', 'general.basename': 'AVA-Llama', 'tokenizer.ggml.padding_token_id': '128257', 'general.architecture': 'llama', 'general.url': 'https://huggingface.co/mradermacher/AVA-Llama-3-V2-i1-GGUF', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantized_by': 'mradermacher', 'general.file_type': '18', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.vocab_size': '128258', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128257', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'mradermacher.quantize_version': '2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: خداحافظ!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "import time  # For adding delay\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [ \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\", \"activate_ib\", \"issuance_card\" ],\n",
    "    \"open_account_current\": [\"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\",  \"activate_ib\" ,\"issuance_card\",  \"shared_cheque\", \"cheque_n\" ,\"support\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\",  \"receiver_card\" ,\"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\",  \"receiver_iban\", \"receiver_bank\" , \"static_pass\",\"trans_periodic\"],\n",
    "    \"convert_cheque\": [ \"transfer_reason\", \"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"cheque_date\", \"transfer_datetime\" , \"static_pass\" ],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\",  \"min_amount\", \"max_amount\" ,\"trans_n\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\",\"name\", \"start_datetime\", \"end_datetime\" , \"advocacy_reason\" ,  \"b-ncid\" ],\n",
    "    \"currency_request\": [\"currency\" , \"amount\", \"country\"  ], \n",
    "    \"loan_free\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\"],\n",
    "    \"open_account_deposit\" : [ \"fname\" , \"lname\" , \"national_id\", \"father_name\" , \"birth_date\", \"address\", \"starter_amount\", \"benefit_rate\",  \"issuance_card\", \"deposit_duration\"]\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Function for intent detection\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"])\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    # Get the confidence score of the detected intent\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    # Get the top 3 intents with their probabilities\n",
    "    top_3_indices = torch.topk(probabilities, 3, dim=1).indices.squeeze().tolist()\n",
    "    top_3_probs = torch.topk(probabilities, 3, dim=1).values.squeeze().tolist()\n",
    "    top_3_intents = intent_label_encoder.inverse_transform(top_3_indices)\n",
    "\n",
    "    print(f\"Detected Intent: {predicted_intent} (Confidence: {confidence:.4f})\")\n",
    "    print(\"Top 3 probable intents:\")\n",
    "    for intent, prob in zip(top_3_intents, top_3_probs):\n",
    "        print(f\"  {intent}: {prob:.4f}\")\n",
    "\n",
    "    return predicted_intent\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/AVA-Llama-3-V2.i1-Q6_K.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=2048)\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder, current_intent):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname).\n",
    "    Map detected slots to intent-specific slots based on the intent.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"o\":\n",
    "            if current_slot == normalized_slot:\n",
    "                # Append to the current slot value (for multi-token slots)\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                # Start a new slot\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    # Map detected slots to intent-specific slots\n",
    "    mapped_slots = {}\n",
    "    intent_slots = intent_slot_mapping.get(current_intent, [])\n",
    "\n",
    "    for slot_name, slot_value in slots.items():\n",
    "        if slot_name in intent_slots:\n",
    "            # Only add valid slots based on the intent\n",
    "            mapped_slots[slot_name] = slot_value\n",
    "        else:\n",
    "            # Handle semantically similar slots (e.g., cheque_reason → transfer_reason)\n",
    "            similar_slot = find_similar_slot(slot_name, intent_slots)\n",
    "            if similar_slot:\n",
    "                mapped_slots[similar_slot] = slot_value\n",
    "\n",
    "    return mapped_slots\n",
    "\n",
    "# Helper function to find a similar slot in the intent's slots\n",
    "def find_similar_slot(detected_slot, intent_slots):\n",
    "    \"\"\"\n",
    "    Find a similar slot in the intent's slots based on semantic similarity.\n",
    "    For example, if the detected slot is 'cheque_reason' and the intent is 'card2card',\n",
    "    map it to 'transfer_reason'.\n",
    "    \"\"\"\n",
    "    # Define semantic groups for similar slots\n",
    "    semantic_groups = {\n",
    "        \"reason\": [\"transfer_reason\", \"cheque_reason\", \"loan_reason\", \"blocking_reason\", \"renew_reason\", \"advocacy_reason\" ],\n",
    "        \"amount\": [\"transfer_amount\", \"loan_amount\", \"installment_amount\", \"cheque_amount\" ,\"min_amount\" ,\"max_amount\" ],\n",
    "        \"datetime\": [\"transfer_datetime\", \"cheque_date\", \"start_datetime\", \"end_datetime\"],\n",
    "        \"card\": [\"card_number\", \"receiver_card\"],\n",
    "        \"name\": [\"fname\", \"lname\", \"cfname\", \"clname\", \"zfname\", \"zlname\"],\n",
    "        \"national_id\" : [\"znational_id\" , \"cnational_id\" , \"national_id\"],\n",
    "         \"duration\" : [\"loan_duration\" ,\"deposit_duration\"]\n",
    "    }\n",
    "\n",
    "    for group, similar_slots in semantic_groups.items():\n",
    "        if detected_slot in similar_slots:\n",
    "            # Find the first matching slot in the intent's slots\n",
    "            for intent_slot in intent_slots:\n",
    "                if intent_slot in similar_slots:\n",
    "                    return intent_slot\n",
    "\n",
    "    # If no similar slot is found, return None\n",
    "    return None\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    " شما یک دستیار چت بات هوشمند بانکی هستید ! از شما میخواهم برای شکاف ها زیر با توجه به دامنه٫ سوال به فارسی تولید کنید و از کاربر بپرسید\n",
    "   ( واژه نامه بعضی واژگان: sayad_id  −>  شناسه صیادی چک # benefit_rate −> نرخ سود بانکی # issuance_card -> درخواست صدور کارت شتاب #  activate_ib -> فعالسازی اینترنت بانک  # trans_n -> تعداد تراکنش های بانکی  #  znational_id -> شماره ملی فرد ضامن  # cnational_id  -> کد ملی گیرنده (حامل) چک  #  \"shared_cheque\"  -> نیازمند دسته چک اشتراکی# \"support\"  -> تعهد (ضمانت) مالی # receiver_iban −> شماره شبای فرد گیرنده)  \n",
    "دامنه بحث  : {intent}\n",
    " شکاف : {slot}\n",
    "\n",
    "یک سوال باید بر اساس دامنه و مختصر و مرتبط باشد و به فارسی و خطاب به کاربر برای دریافت اطلاعات پرسیده شود.\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=90, temperature=0.75, top_p=0.5)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    unfilled_slots = []\n",
    "    last_asked_slot = None\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            unfilled_slots = intent_slot_mapping.get(current_intent, [])\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder, current_intent)\n",
    "        for slot_name, slot_value in slots.items():\n",
    "            # Update filled_slots and remove from unfilled_slots\n",
    "            if slot_name in intent_slot_mapping[current_intent]:  # Only add valid slots\n",
    "                filled_slots[slot_name] = slot_value\n",
    "                if slot_name in unfilled_slots:\n",
    "                    unfilled_slots.remove(slot_name)\n",
    "\n",
    "        # Print filled and unfilled slots\n",
    "        print(\"چت‌بات: وضعیت فعلی:\")\n",
    "        print(\"پر شده:\")\n",
    "        print(json.dumps(filled_slots, ensure_ascii=False, indent=4))\n",
    "        print(\"پر نشده:\")\n",
    "        print(json.dumps(unfilled_slots, ensure_ascii=False, indent=4))\n",
    "\n",
    "        # Step 3: Ask about the next unfilled slot\n",
    "        if unfilled_slots:\n",
    "            next_slot = unfilled_slots[0]\n",
    "            last_asked_slot = next_slot\n",
    "\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_89399/1601704940.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot-Filling Model Test\n",
      "Type your input (or type 'exit' to quit):\n",
      "Detected Slots:\n",
      "  loan_benefit_rate: ۵ درصد\n",
      "Detected Slots:\n",
      "  loan_benefit_rate: ۵ درصد\n",
      "Detected Slots:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Initialize slot label encoder\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "# Load the slot-filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "slot_model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "\n",
    "# Function to predict slots\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname).\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"o\":\n",
    "            if current_slot == normalized_slot:\n",
    "                # Append to the current slot value (for multi-token slots)\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                # Start a new slot\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Main function for testing slot filling\n",
    "def test_slot_filling():\n",
    "    print(\"Slot-Filling Model Test\")\n",
    "    print(\"Type your input (or type 'exit' to quit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Predict slots\n",
    "        detected_slots = predict_slots(slot_model, slot_tokenizer, user_input, slot_label_encoder)\n",
    "        \n",
    "        # Print detected slots\n",
    "        print(\"Detected Slots:\")\n",
    "        for slot, value in detected_slots.items():\n",
    "            print(f\"  {slot}: {value}\")\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_slot_filling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Frequencies:\n",
      "loan_interest: 72\n",
      "open_account_free: 92\n",
      "duplicate_card: 119\n",
      "change_password: 120\n",
      "delegate_account: 90\n",
      "software_problem: 110\n",
      "installment_payment: 90\n",
      "paya: 92\n",
      "submit_cheque: 90\n",
      "signin_problem: 71\n",
      "card2card: 88\n",
      "convert_cheque: 88\n",
      "open_account_current: 79\n",
      "currency_request: 90\n",
      "turnover_bill: 90\n",
      "recieve_cheque: 120\n",
      "close_card: 120\n",
      "balance_bill: 90\n",
      "receipt_payment: 79\n",
      "open_account_deposit: 74\n",
      "loan_free: 78\n",
      "\n",
      "Occurrences of 'b-transfer_reason' in intent 'card2card': 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from the given file path.\n",
    "    Each line contains a sentence and its intent separated by '<=>'.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "# Step 2: Extract intents and slot labels\n",
    "def extract_intents_and_slots(lines):\n",
    "    \"\"\"\n",
    "    Extract intents and slot labels from the dataset.\n",
    "    \"\"\"\n",
    "    intents = []\n",
    "    slots_per_intent = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # Split the line into the sentence and the intent\n",
    "        sentence, intent = line.strip().split(\" <=> \")\n",
    "        intents.append(intent)\n",
    "\n",
    "        # Extract slots (e.g., b-transfer_reason, i-transfer_reason, etc.)\n",
    "        slots = re.findall(r\"\\b(b|i)-\\w+\", sentence)\n",
    "\n",
    "        # Group slots by intent\n",
    "        if intent not in slots_per_intent:\n",
    "            slots_per_intent[intent] = []\n",
    "        slots_per_intent[intent].extend(slots)\n",
    "\n",
    "    return intents, slots_per_intent\n",
    "\n",
    "# Step 3: Count intent frequencies\n",
    "def count_intent_frequencies(intents):\n",
    "    \"\"\"\n",
    "    Count the frequency of each intent in the dataset.\n",
    "    \"\"\"\n",
    "    intent_counts = Counter(intents)\n",
    "    return intent_counts\n",
    "\n",
    "# Step 4: Count specific slot occurrences for a given intent\n",
    "def count_slot_in_intent(slots_per_intent, target_intent, target_slot):\n",
    "    \"\"\"\n",
    "    Count the occurrences of a specific slot (e.g., b-transfer_reason) in a given intent.\n",
    "    \"\"\"\n",
    "    if target_intent not in slots_per_intent:\n",
    "        return 0\n",
    "    return slots_per_intent[target_intent].count(target_slot)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Path to the train.txt file\n",
    "    file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/train.txt\"\n",
    "\n",
    "    # Load the dataset\n",
    "    lines = load_dataset(file_path)\n",
    "\n",
    "    # Extract intents and slots\n",
    "    intents, slots_per_intent = extract_intents_and_slots(lines)\n",
    "\n",
    "    # Step 1: Count the frequency of different intents\n",
    "    intent_counts = count_intent_frequencies(intents)\n",
    "    print(\"Intent Frequencies:\")\n",
    "    for intent, count in intent_counts.items():\n",
    "        print(f\"{intent}: {count}\")\n",
    "\n",
    "    # Step 2: Count occurrences of 'b-transfer_reason' in the intent 'card2card'\n",
    "    target_intent = \"card2card\"\n",
    "    target_slot = \"b-transfer_reason\"\n",
    "    slot_count = count_slot_in_intent(slots_per_intent, target_intent, target_slot)\n",
    "    print(f\"\\nOccurrences of '{target_slot}' in intent '{target_intent}': {slot_count}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Initialize LabelEncoder for intents\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Downloads/best_augmented_intent_model\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "intent_model.to(device)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict the intent of the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "        model: Trained intent detection model.\n",
    "        intent_label_encoder: LabelEncoder for intent labels.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted intent label.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Main function to test intent detection\n",
    "def main():\n",
    "    print(\"Intent Detection Test\")\n",
    "    while True:\n",
    "        user_message = input(\"Enter your message (or type 'exit' to quit): \")\n",
    "        if user_message.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Predict intent\n",
    "        intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "        print(f\"Predicted Intent: {intent}\")\n",
    "\n",
    "# Run the intent detection test\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
