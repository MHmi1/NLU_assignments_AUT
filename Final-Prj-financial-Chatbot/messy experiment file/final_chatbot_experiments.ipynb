{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_130573/2368146236.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cpu\") ) , strict=False  )\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Final_Llama3_Part\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Final_Llama3_Part\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.context_length': '8192', 'general.name': 'Final_Llama3_Part', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: نیت شما شناسایی شد: paya\n",
      "چت‌بات: اطلاعات استخراج‌شده: {'o': 'است', 'b-fname': 'محسن', 'b-lname': 'حامی', 'b-national_id': '۳۸۰۱۲۴۵۸۹۶'}\n",
      "چت‌بات: شناسایی اسلات‌های پر نشده...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   36041.53 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   419 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   69652.94 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: سوالات تولید شده برای اسلات‌های پر نشده:\n",
      " همچنین، لطفاً به عنوان پیش فرض، برای هر سوال گزینه‌های \"بله\" و \"خیر\" ارائه دهید.assistant\n",
      "\n",
      "I'm excited to help you generate questions for each of the \"es\" - e.g. \"b-account_id\", \"b-activate_ib\", \"b-address\", etc.!\n",
      "\n",
      "Please let me know if you'd like me to create yes/no questions for each of these fields, or if\n",
      "چت‌بات: نیت شما شناسایی شد: loan_interest\n",
      "چت‌بات: اطلاعات استخراج‌شده: {'o': 'بدم', 'b-loan_amount': '۱۰۰۰', 'i-loan_amount': 'تومن', 'b-name': 'بانک', 'i-receiver_bank': 'مرکزی', 'b-cheque_reason': 'قسط', 'i-cheque_reason': 'دانشجویی'}\n",
      "چت‌بات: شناسایی اسلات‌های پر نشده...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 42 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   36041.53 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   376 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   66002.84 ms /   469 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: سوالات تولید شده برای اسلات‌های پر نشده:\n",
      "assistant\n",
      "\n",
      "I'd be happy to help! For each of the fields that need to be populated, I'll create a sample question that can be used to gather the necessary information. Here are the questions:\n",
      "\n",
      "1. b-account_id: What is your bank account number?\n",
      "2. b-activate_ib: Are you interested in activating an international bank account?\n",
      "3. b-address: What is your current mailing address?\n",
      "4. b-advoc\n",
      "چت‌بات: خداحافظ!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = AutoModelForTokenClassification.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\", num_labels=len(slot_labels))\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cpu\") ) , strict=False  )\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")  # Adjust tokenizer if different\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=512)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict the intent of the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "        model: Trained intent detection model.\n",
    "        intent_label_encoder: LabelEncoder for intent labels.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted intent label.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict slots for a given input text.\n",
    "\n",
    "    Args:\n",
    "        model: Trained slot filling model.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "        text (str): Input text.\n",
    "        slot_label_encoder: LabelEncoder for slot labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Predicted slots with their corresponding words.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Predict slot labels\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Align predictions with input tokens\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    # Map slots to words\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        if slot != \"O\":  # Skip non-slot words\n",
    "            slots[slot] = word\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Function to generate questions for unfilled slots using LLaMA\n",
    "def generate_questions_with_llm(intent, unfilled_slots):\n",
    "    \"\"\"\n",
    "    Generate questions for unfilled slots using LLaMA.\n",
    "\n",
    "    Args:\n",
    "        intent (str): Detected intent.\n",
    "        unfilled_slots (list): List of unfilled slots.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated questions in Persian.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "شما یک دستیار بانکی هستید که باید سوالاتی به زبان فارسی تولید کنید. در اینجا نیت و اطلاعات اسلات‌های پر نشده آورده شده است:\n",
    "\n",
    "نیت: {intent}\n",
    "اسلات‌های پر نشده:\n",
    "\"\"\"\n",
    "    for slot in unfilled_slots:\n",
    "        prompt += f\"- {slot}\\n\"\n",
    "\n",
    "    prompt += \"\\nلطفاً برای هر اسلات یک سوال مناسب تولید کنید.\"\n",
    "\n",
    "    # Send the prompt to LLaMA\n",
    "    output = llm(prompt, max_tokens=200)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent\n",
    "        intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "        print(f\"چت‌بات: نیت شما شناسایی شد: {intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder)\n",
    "        print(f\"چت‌بات: اطلاعات استخراج‌شده: {slots}\")\n",
    "\n",
    "        # Step 3: Identify unfilled slots\n",
    "        unfilled_slots = [slot for slot in slot_labels if slot.startswith(\"b-\") and slot not in slots]\n",
    "        if unfilled_slots:\n",
    "            print(\"چت‌بات: شناسایی اسلات‌های پر نشده...\")\n",
    "            questions = generate_questions_with_llm(intent, unfilled_slots)\n",
    "            print(\"چت‌بات: سوالات تولید شده برای اسلات‌های پر نشده:\")\n",
    "            print(questions)\n",
    "        else:\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. درخواست شما ثبت شد.\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_22367/2690239064.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Final_Llama3_Part\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "Exception ignored on calling ctypes callback function: <function llama_log_callback at 0x754be6fd71a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mh/anaconda3/lib/python3.12/site-packages/llama_cpp/_logger.py\", line 39, in llama_log_callback\n",
      "    print(text.decode(\"utf-8\"), end=\"\", flush=True, file=sys.stderr)\n",
      "  File \"/home/mh/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py\", line 578, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mh/anaconda3/lib/python3.12/threading.py\", line 652, in wait\n",
      "    with self._cond:\n",
      "  File \"/home/mh/anaconda3/lib/python3.12/threading.py\", line 302, in __exit__\n",
      "    def __exit__(self, *args):\n",
      "\n",
      "KeyboardInterrupt: \n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Final_Llama3_Part\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.context_length': '8192', 'general.name': 'Final_Llama3_Part', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: نیت شما شناسایی شد: receipt_payment\n",
      "چت‌بات: اطلاعات استخراج‌شده: {'b-country': 'خورج'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\"],\n",
    "    \"open_account_current\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\", \"support\", \"cheque_n\", \"shared_cheque\"],\n",
    "    \"open_account_deposit\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\", \"benefit_rate\", \"deposit_duration\"],\n",
    "    \"loan_free\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\"],\n",
    "    \"loan_interest\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\", \"loan_benefit_rate\", \"loan_support\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\", \"receiver_card\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"static_pass\", \"trans_periodic\", \"receiver_iban\", \"receiver_bank\"],\n",
    "    \"convert_cheque\": [\"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"transfer_reason\", \"static_pass\", \"cheque_date\", \"transfer_datetime\"],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"trans_n\", \"min_amount\", \"max_amount\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"name\", \"b-ncid\", \"advocacy_reason\"],\n",
    "    \"currency_request\": [\"country\", \"amount\", \"currency\"]\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=512)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder):\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        if slot != \"O\":\n",
    "            slots[slot] = word\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    "Below are examples of how to generate questions in Persian for unfilled slots:\n",
    "\n",
    "Example 1:\n",
    "Intent: - open_account_free\n",
    "Slots:\n",
    "fname: filled\n",
    "lname: filled\n",
    "national_id: null\n",
    "address: null\n",
    "\n",
    "Generate a question in Persian for the first unfilled slot ( national_id slot ):\n",
    "لطفاً کد ملی خود را وارد کنید.\n",
    "\n",
    "Now generate a question for the following slot based on the intent:\n",
    "Intent: - {intent}\n",
    "Slot: - {slot}\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    intent_change_attempts = 0\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "        if current_intent is None:\n",
    "            current_intent = intent\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "        elif intent != current_intent:\n",
    "            intent_change_attempts += 1\n",
    "            print(\"چت‌بات: شما در حال حاضر نمی‌توانید نیت جدیدی را شروع کنید. لطفاً اطلاعات مربوط به نیت فعلی را تکمیل کنید.\")\n",
    "            if intent_change_attempts >= 3:\n",
    "                print(\"چت‌بات: چت بسته شد. لطفاً دوباره تلاش کنید.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder)\n",
    "        filled_slots.update(slots)\n",
    "        print(f\"چت‌بات: اطلاعات استخراج‌شده: {filled_slots}\")\n",
    "\n",
    "        # Step 3: Identify unfilled slots\n",
    "        required_slots = intent_slot_mapping.get(current_intent, [])\n",
    "        unfilled_slots = [slot for slot in required_slots if slot not in filled_slots]\n",
    "\n",
    "        if unfilled_slots:\n",
    "            # Ask about the first unfilled slot\n",
    "            next_slot = unfilled_slots[0]\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_22367/4264640829.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Final_Llama3_Part\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Final_Llama3_Part\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.context_length': '8192', 'general.name': 'Final_Llama3_Part', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: نیت شما شناسایی شد: open_account_current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1424.58 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   75960.95 ms /   284 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1424.58 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   56320.73 ms /   190 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: - لطفاً یک کارت اعتباری درخواست دهید\n",
      "سپس اسلات تکمیل می شود و کاربر می تواند در مورد درخواستش برای کارت اعتباری اطلاعات را تکمیل کند.assistant\n",
      "\n",
      "It seems like we have a scenario set up! 😊\n",
      "\n",
      "So, the user is asking to open a current account and has requested an issuance of a card. I'll make sure to respond accordingly. Here's a possible response:\n",
      "\n",
      "\"Thank you for your request! I'd be happy to assist you with opening a current account and issuing a card. To proceed, could you please provide me with the following information: \n",
      "\n",
      "1. Your full name\n",
      "2. Date of birth\n",
      "3. Address (residential and/or business)\n",
      "4. Contact information (email, phone number, etc.)\n",
      "\n",
      "Please let me know if you have any questions or concerns. I'll be happy to guide you through the process.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 173\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     chatbot()\n",
      "Cell \u001b[0;32mIn[2], line 161\u001b[0m, in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unfilled_slots:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Ask about the first unfilled slot\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     next_slot \u001b[38;5;241m=\u001b[39m unfilled_slots[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m     question \u001b[38;5;241m=\u001b[39m generate_question_with_llm(current_intent, next_slot)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mچت‌بات: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# All slots are filled, send \"Done!\" message and return JSON\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 130\u001b[0m, in \u001b[0;36mgenerate_question_with_llm\u001b[0;34m(intent, slot)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_question_with_llm\u001b[39m(intent, slot):\n\u001b[1;32m    124\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124mIntent: - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mintent\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124mSlot: - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslot\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124mبرای اولین اسلات تکمیل نشده بالا یک سوال پرسشی ایجاد کن :\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 130\u001b[0m     output \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1899\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1862\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1864\u001b[0m \n\u001b[1;32m   1865\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[1;32m   1900\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1901\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m   1902\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   1903\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1904\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1905\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1906\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1907\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m   1908\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m   1909\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   1910\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1911\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1912\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1913\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1914\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1915\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1916\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1917\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1918\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1919\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1920\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1921\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1922\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1923\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1924\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1925\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1832\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1318\u001b[0m     prompt_tokens,\n\u001b[1;32m   1319\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1320\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1321\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1322\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1323\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1324\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1325\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1326\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1327\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1328\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1329\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1330\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1331\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1332\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1333\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1334\u001b[0m ):\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n\u001b[1;32m   1336\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    911\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    912\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    913\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    928\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[1;32m    302\u001b[0m         batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\"],\n",
    "    \"open_account_current\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\", \"support\", \"cheque_n\", \"shared_cheque\"],\n",
    "    \"open_account_deposit\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\", \"benefit_rate\", \"deposit_duration\"],\n",
    "    \"loan_free\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\"],\n",
    "    \"loan_interest\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\", \"loan_benefit_rate\", \"loan_support\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\", \"receiver_card\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"static_pass\", \"trans_periodic\", \"receiver_iban\", \"receiver_bank\"],\n",
    "    \"convert_cheque\": [\"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"transfer_reason\", \"static_pass\", \"cheque_date\", \"transfer_datetime\"],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"trans_n\", \"min_amount\", \"max_amount\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"name\", \"b-ncid\", \"advocacy_reason\"],\n",
    "    \"currency_request\": [\"country\", \"amount\", \"currency\"]\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=512)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder):\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        if slot != \"O\":\n",
    "            slots[slot] = word\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    "Intent: - {intent}\n",
    "Slot: - {slot}\n",
    "\n",
    "برای اولین اسلات تکمیل نشده بالا یک سوال پرسشی ایجاد کن :\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder)\n",
    "        filled_slots.update(slots)\n",
    "\n",
    "        # Step 3: Identify unfilled slots\n",
    "        required_slots = intent_slot_mapping.get(current_intent, [])\n",
    "        unfilled_slots = [slot for slot in required_slots if slot not in filled_slots]\n",
    "\n",
    "        if unfilled_slots:\n",
    "            # Ask about the first unfilled slot\n",
    "            next_slot = unfilled_slots[0]\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, send \"Done!\" message and return JSON\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            print(\"چت‌بات: Done!\")\n",
    "            return result  # Return the result JSON structure\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_46910/4266940276.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Final_Llama3_Part\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Final_Llama3_Part\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.context_length': '8192', 'general.name': 'Final_Llama3_Part', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: نیت شما شناسایی شد: card2card\n",
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{\n",
      "    \"o\": \"بنام\",\n",
      "    \"card_number\": \"۵۸۵۹۸۳۱۱۷۵۲۲۴۴۸۸۹\",\n",
      "    \"receiver_card\": \"۴۵۷۷۸۹۶۶۳۳۲۲۰۲۱۴\",\n",
      "    \"fname\": \"ج\",\n",
      "    \"lname\": \"حامی\"\n",
      "}\n",
      "پر نشده:\n",
      "[\n",
      "    \"transfer_amount\",\n",
      "    \"transfer_datetime\",\n",
      "    \"transfer_reason\",\n",
      "    \"cvv2\",\n",
      "    \"trans_pass\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4190.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   37392.39 ms /   212 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: slots:\n",
      "amount: null\n",
      "from_card: filled\n",
      "to_card: filled\n",
      "\n",
      "Generate a question in Persian for the amount slot:\n",
      "مبلغی را برای انتقال انتخاب کنید.\n",
      "\n",
      "Example 3:\n",
      "Intent: - card2card\n",
      "Slot: - transfer_card\n",
      "slots:\n",
      "card: null\n",
      "amount: filled\n",
      "from_card: filled\n",
      "to_card: filled\n",
      "\n",
      "Generate a question in Persian for the card slot:\n",
      "کارت را برای انتقال انتخاب کنید.assistant\n",
      "\n",
      "I cannot provide you with the best way to generate a question in Persian for the slot.\n",
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{\n",
      "    \"o\": \"در\",\n",
      "    \"card_number\": \"۵۸۵۹۸۳۱۱۷۵۲۲۴۴۸۸۹\",\n",
      "    \"receiver_card\": \"۴۵۷۷۸۹۶۶۳۳۲۲۰۲۱۴\",\n",
      "    \"fname\": \"ج\",\n",
      "    \"lname\": \"حامی\",\n",
      "    \"min_amount\": \"۲۵۰۰\",\n",
      "    \"cheque_amount\": \"تومن\",\n",
      "    \"cheque_reason\": \"صندلی\",\n",
      "    \"transfer_datetime\": \"جمعه\"\n",
      "}\n",
      "پر نشده:\n",
      "[\n",
      "    \"transfer_amount\",\n",
      "    \"transfer_reason\",\n",
      "    \"cvv2\",\n",
      "    \"trans_pass\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 98 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4190.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   43306.12 ms /   149 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: Slot value: 1000.0\n",
      "Example dialogue:\n",
      "- لطفاً مبلغ انتقال را وارد کنید.assistant\n",
      "\n",
      "I'd be happy to help you with your request! However, I need more information from you. Please provide more context or clarify what you mean by \"generate questions in Persian for unfilled slots\". What kind of slots are you referring to? Are they related to a specific task or application?\n",
      "\n",
      "For example, are you trying to generate questions for a conversational AI chatbot or a survey? Are the slots related to personal information, financial data, or something else?\n",
      "\n",
      "Once I have more information, I'll do my best to assist you in generating questions in Persian for unfilled slots.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\"],\n",
    "    \"open_account_current\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\", \"support\", \"cheque_n\", \"shared_cheque\"],\n",
    "    \"open_account_deposit\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\", \"benefit_rate\", \"deposit_duration\"],\n",
    "    \"loan_free\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\"],\n",
    "    \"loan_interest\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\", \"loan_benefit_rate\", \"loan_support\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\", \"receiver_card\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"static_pass\", \"trans_periodic\", \"receiver_iban\", \"receiver_bank\"],\n",
    "    \"convert_cheque\": [\"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"transfer_reason\", \"static_pass\", \"cheque_date\", \"transfer_datetime\"],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"trans_n\", \"min_amount\", \"max_amount\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"name\", \"b-ncid\", \"advocacy_reason\"],\n",
    "    \"currency_request\": [\"country\", \"amount\", \"currency\"]\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=512)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"])\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname, I-fname → fname).\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"O\":\n",
    "            slots[normalized_slot] = word\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    "Below are examples of how to generate questions in Persian for unfilled slots:\n",
    "\n",
    "Example 1:\n",
    "Intent: - open_account_free\n",
    "Slots:\n",
    "fname: filled\n",
    "lname: filled\n",
    "national_id: null\n",
    "address: null\n",
    "\n",
    "Generate a question in Persian for the first unfilled slot ( national_id slot ):\n",
    "لطفاً کد ملی خود را وارد کنید.\n",
    "\n",
    "Now generate a question for the following slot based on the intent:\n",
    "Intent: - {intent}\n",
    "Slot: - {slot}\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    unfilled_slots = []\n",
    "    last_asked_slot = None\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            unfilled_slots = intent_slot_mapping.get(current_intent, [])\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder)\n",
    "        for slot_name, slot_value in slots.items():\n",
    "            # Update filled_slots and remove from unfilled_slots\n",
    "            filled_slots[slot_name] = slot_value\n",
    "            if slot_name in unfilled_slots:\n",
    "                unfilled_slots.remove(slot_name)\n",
    "\n",
    "        # Print filled and unfilled slots\n",
    "        print(\"چت‌بات: وضعیت فعلی:\")\n",
    "        print(\"پر شده:\")\n",
    "        print(json.dumps(filled_slots, ensure_ascii=False, indent=4))\n",
    "        print(\"پر نشده:\")\n",
    "        print(json.dumps(unfilled_slots, ensure_ascii=False, indent=4))\n",
    "\n",
    "        # Step 3: Ask about the next unfilled slot\n",
    "        if unfilled_slots:\n",
    "            # If the user changes intent, continue asking about the previous slot\n",
    "            if last_asked_slot in unfilled_slots:\n",
    "                next_slot = last_asked_slot\n",
    "            else:\n",
    "                next_slot = unfilled_slots[0]\n",
    "                last_asked_slot = next_slot\n",
    "\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_48934/2064500612.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: نیت شما شناسایی شد: duplicate_card\n",
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{\n",
      "    \"o\": \"سلام میخواهم کارت جدید دریافت کنم چون کارت قبلیم رو گم کرده ام\"\n",
      "}\n",
      "پر نشده:\n",
      "[\n",
      "    \"card_number\",\n",
      "    \"renew_reason\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 97 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 242\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 242\u001b[0m     chatbot()\n",
      "Cell \u001b[0;32mIn[14], line 231\u001b[0m, in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m         next_slot \u001b[38;5;241m=\u001b[39m unfilled_slots[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m         last_asked_slot \u001b[38;5;241m=\u001b[39m next_slot\n\u001b[0;32m--> 231\u001b[0m     question \u001b[38;5;241m=\u001b[39m generate_question_with_llm(current_intent, next_slot)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mچت‌بات: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# All slots are filled, return JSON and exit\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 184\u001b[0m, in \u001b[0;36mgenerate_question_with_llm\u001b[0;34m(intent, slot)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_question_with_llm\u001b[39m(intent, slot):\n\u001b[1;32m    166\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124mBelow are examples of how to generate questions in Persian for unfilled slots:\u001b[39m\n\u001b[1;32m    168\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124mSlot: - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslot\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 184\u001b[0m     output \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1899\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1862\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1864\u001b[0m \n\u001b[1;32m   1865\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[1;32m   1900\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1901\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m   1902\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   1903\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1904\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1905\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1906\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1907\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m   1908\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m   1909\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   1910\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1911\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1912\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1913\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1914\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1915\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1916\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1917\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1918\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1919\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1920\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1921\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1922\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1923\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1924\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1925\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1832\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1318\u001b[0m     prompt_tokens,\n\u001b[1;32m   1319\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1320\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1321\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1322\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1323\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1324\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1325\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1326\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1327\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1328\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1329\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1330\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1331\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1332\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1333\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1334\u001b[0m ):\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n\u001b[1;32m   1336\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    911\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    912\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    913\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    928\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[1;32m    302\u001b[0m         batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\"],\n",
    "    \"open_account_current\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\", \"support\", \"cheque_n\", \"shared_cheque\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\", \"receiver_card\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"static_pass\", \"trans_periodic\", \"receiver_iban\", \"receiver_bank\"],\n",
    "    \"convert_cheque\": [\"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"transfer_reason\", \"static_pass\", \"cheque_date\", \"transfer_datetime\"],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"trans_n\", \"min_amount\", \"max_amount\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"name\", \"b-ncid\", \"advocacy_reason\"],\n",
    "    \"currency_request\": [\"country\", \"amount\", \"currency\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"])\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder, current_intent):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname).\n",
    "    Map detected slots to intent-specific slots based on the intent.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"O\":\n",
    "            if current_slot == normalized_slot:\n",
    "                # Append to the current slot value (for multi-token slots)\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                # Start a new slot\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    # Map detected slots to intent-specific slots\n",
    "    mapped_slots = {}\n",
    "    intent_slots = intent_slot_mapping.get(current_intent, [])\n",
    "\n",
    "    for slot_name, slot_value in slots.items():\n",
    "        if slot_name in intent_slots:\n",
    "            # If the detected slot is valid for the current intent, keep it as is\n",
    "            mapped_slots[slot_name] = slot_value\n",
    "        else:\n",
    "            # Handle semantically similar slots (e.g., cheque_reason → transfer_reason)\n",
    "            similar_slot = find_similar_slot(slot_name, intent_slots)\n",
    "            if similar_slot:\n",
    "                mapped_slots[similar_slot] = slot_value\n",
    "            else:\n",
    "                # If no similar slot is found, keep the original slot\n",
    "                mapped_slots[slot_name] = slot_value\n",
    "\n",
    "    return mapped_slots\n",
    "\n",
    "# Helper function to find a similar slot in the intent's slots\n",
    "def find_similar_slot(detected_slot, intent_slots):\n",
    "    \"\"\"\n",
    "    Find a similar slot in the intent's slots based on semantic similarity.\n",
    "    For example, if the detected slot is 'cheque_reason' and the intent is 'card2card',\n",
    "    map it to 'transfer_reason'.\n",
    "    \"\"\"\n",
    "    # Define semantic groups for similar slots\n",
    "    semantic_groups = {\n",
    "        \"reason\": [\"transfer_reason\", \"cheque_reason\", \"loan_reason\" ,\"renew_reason\" , \"blocking_reason\"],\n",
    "        \"amount\": [\"transfer_amount\", \"loan_amount\", \"installment_amount\", \"cheque_amount\"],\n",
    "        \"datetime\": [\"transfer_datetime\", \"cheque_date\", \"start_datetime\", \"end_datetime\"],\n",
    "        \"card\": [\"card_number\", \"receiver_card\"],\n",
    "        \"name\": [\"fname\", \"lname\", \"cfname\", \"clname\", \"zfname\", \"zlname\"],\n",
    "    }\n",
    "\n",
    "    for group, similar_slots in semantic_groups.items():\n",
    "        if detected_slot in similar_slots:\n",
    "            # Find the first matching slot in the intent's slots\n",
    "            for intent_slot in intent_slots:\n",
    "                if intent_slot in similar_slots:\n",
    "                    return intent_slot\n",
    "\n",
    "    # If no similar slot is found, return None\n",
    "    return None\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    "Below are examples of how to generate questions in Persian for unfilled slots:\n",
    "\n",
    "Example 1:\n",
    "Intent: - open_account_free\n",
    "Slots:\n",
    "fname: filled\n",
    "lname: filled\n",
    "national_id: null\n",
    "address: null\n",
    "\n",
    "Generate a question in Persian for the first unfilled slot ( national_id slot ):\n",
    "لطفاً کد ملی خود را وارد کنید.\n",
    "\n",
    "Now generate a question for the following slot based on the intent:\n",
    "Intent: - {intent}\n",
    "Slot: - {slot}\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    unfilled_slots = []\n",
    "    last_asked_slot = None\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            unfilled_slots = intent_slot_mapping.get(current_intent, [])\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder, current_intent)\n",
    "        for slot_name, slot_value in slots.items():\n",
    "            # Update filled_slots and remove from unfilled_slots\n",
    "            filled_slots[slot_name] = slot_value\n",
    "            if slot_name in unfilled_slots:\n",
    "                unfilled_slots.remove(slot_name)\n",
    "\n",
    "        # Print filled and unfilled slots\n",
    "        print(\"چت‌بات: وضعیت فعلی:\")\n",
    "        print(\"پر شده:\")\n",
    "        print(json.dumps(filled_slots, ensure_ascii=False, indent=4))\n",
    "        print(\"پر نشده:\")\n",
    "        print(json.dumps(unfilled_slots, ensure_ascii=False, indent=4))\n",
    "\n",
    "        # Step 3: Ask about the next unfilled slot\n",
    "        if unfilled_slots:\n",
    "            # If the user changes intent, continue asking about the previous slot\n",
    "            if last_asked_slot in unfilled_slots:\n",
    "                next_slot = last_asked_slot\n",
    "            else:\n",
    "                next_slot = unfilled_slots[0]\n",
    "                last_asked_slot = next_slot\n",
    "\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_79375/2073568983.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Final_Llama3_Part\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Final_Llama3_Part\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.context_length': '8192', 'general.name': 'Final_Llama3_Part', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: نیت شما شناسایی شد: delegate_account\n",
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{}\n",
      "پر نشده:\n",
      "[\n",
      "    \"account_id\",\n",
      "    \"start_datetime\",\n",
      "    \"end_datetime\",\n",
      "    \"name\",\n",
      "    \"b-ncid\",\n",
      "    \"advocacy_reason\"\n",
      "]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 226\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 226\u001b[0m     chatbot()\n",
      "Cell \u001b[0;32mIn[6], line 215\u001b[0m, in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         next_slot \u001b[38;5;241m=\u001b[39m unfilled_slots[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    213\u001b[0m         last_asked_slot \u001b[38;5;241m=\u001b[39m next_slot\n\u001b[0;32m--> 215\u001b[0m     question \u001b[38;5;241m=\u001b[39m generate_question_with_llm(current_intent, next_slot)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mچت‌بات: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# All slots are filled, return JSON and exit\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 160\u001b[0m, in \u001b[0;36mgenerate_question_with_llm\u001b[0;34m(intent, slot)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_question_with_llm\u001b[39m(intent, slot):\n\u001b[1;32m    141\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124mBelow are examples of how to generate questions in Persian for unfilled slots:\u001b[39m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124mSlot: - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslot\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 160\u001b[0m     output \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1899\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1862\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1864\u001b[0m \n\u001b[1;32m   1865\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[1;32m   1900\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1901\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m   1902\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   1903\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1904\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1905\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1906\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1907\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m   1908\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m   1909\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   1910\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1911\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1912\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1913\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1914\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1915\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1916\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1917\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1918\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1919\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1920\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1921\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1922\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1923\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1924\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1925\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1832\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1318\u001b[0m     prompt_tokens,\n\u001b[1;32m   1319\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1320\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1321\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1322\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1323\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1324\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1325\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1326\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1327\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1328\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1329\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1330\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1331\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1332\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1333\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1334\u001b[0m ):\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n\u001b[1;32m   1336\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    911\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    912\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    913\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    928\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[1;32m    302\u001b[0m         batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "import time  # For adding delay\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\"],\n",
    "    \"open_account_current\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\", \"support\", \"cheque_n\", \"shared_cheque\"],\n",
    "    #\"open_account_deposit\" : [\"issuance_card\", \"fname\" , \"lname\" , \"national_id\", \"father_name\" , \"birth_date\", \"address\", \"starter_amount\", \"benefit_rate\", \"deposit_duration\"] ,\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\", \"receiver_card\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"static_pass\", \"trans_periodic\", \"receiver_iban\", \"receiver_bank\"],\n",
    "    \"convert_cheque\": [\"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"transfer_reason\", \"static_pass\", \"cheque_date\", \"transfer_datetime\"],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"trans_n\", \"min_amount\", \"max_amount\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"name\", \"b-ncid\", \"advocacy_reason\"],\n",
    "    \"currency_request\": [\"country\", \"amount\", \"currency\"]\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=512)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"])\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder, current_intent):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname).\n",
    "    Map detected slots to intent-specific slots based on the intent.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"o\":\n",
    "            if current_slot == normalized_slot:\n",
    "                # Append to the current slot value (for multi-token slots)\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                # Start a new slot\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    # Map detected slots to intent-specific slots\n",
    "    mapped_slots = {}\n",
    "    intent_slots = intent_slot_mapping.get(current_intent, [])\n",
    "\n",
    "    for slot_name, slot_value in slots.items():\n",
    "        if slot_name in intent_slots:\n",
    "            # If the detected slot is valid for the current intent, keep it as is\n",
    "            mapped_slots[slot_name] = slot_value\n",
    "        else:\n",
    "            # Ignore unrelated slots (e.g., \"o\" or \"current_pass\" in card2card)\n",
    "            continue\n",
    "\n",
    "    return mapped_slots\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    "Below are examples of how to generate questions in Persian for unfilled slots:\n",
    "\n",
    "Example 1:\n",
    "Intent: - open_account_free\n",
    "Slots:\n",
    "fname: filled\n",
    "lname: filled\n",
    "national_id: null\n",
    "address: null\n",
    "\n",
    "Generate a question in Persian for the first unfilled slot ( national_id slot ):\n",
    "لطفاً کد ملی خود را وارد کنید.\n",
    "\n",
    "Now generate a question for the following slot based on the intent:\n",
    "Intent: - {intent}\n",
    "Slot: - {slot}\n",
    "\"\"\"\n",
    "    \n",
    "    output = llm(prompt, max_tokens=256)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    unfilled_slots = []\n",
    "    last_asked_slot = None\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            unfilled_slots = intent_slot_mapping.get(current_intent, [])\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Handle bot_challenge intent\n",
    "        if current_intent == \"bot_challenge\":\n",
    "            print(\"چت‌بات: من یک چت بات مالی هستم لطفا سوال مرتبط بپرسید\")\n",
    "            continue  # Skip slot filling and ask the previous question again\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder, current_intent)\n",
    "        for slot_name, slot_value in slots.items():\n",
    "            # Update filled_slots and remove from unfilled_slots\n",
    "            filled_slots[slot_name] = slot_value\n",
    "            if slot_name in unfilled_slots:\n",
    "                unfilled_slots.remove(slot_name)\n",
    "\n",
    "        # Print filled and unfilled slots\n",
    "        print(\"چت‌بات: وضعیت فعلی:\")\n",
    "        print(\"پر شده:\")\n",
    "        print(json.dumps(filled_slots, ensure_ascii=False, indent=4))\n",
    "        print(\"پر نشده:\")\n",
    "        print(json.dumps(unfilled_slots, ensure_ascii=False, indent=4))\n",
    "\n",
    "        # Step 3: Ask about the next unfilled slot\n",
    "        if unfilled_slots:\n",
    "            # Add a delay before asking the next question\n",
    "            time.sleep(2)  # 2-second delay\n",
    "\n",
    "            # If the user changes intent, continue asking about the previous slot\n",
    "            if last_asked_slot in unfilled_slots:\n",
    "                next_slot = last_asked_slot\n",
    "            else:\n",
    "                next_slot = unfilled_slots[0]\n",
    "                last_asked_slot = next_slot\n",
    "\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_31167/3108691314.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 291 tensors from /home/mh/Desktop/AVA-Llama-3-V2.i1-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = AVA Llama 3\n",
      "llama_model_loader: - kv   3:                            general.version str              = 3\n",
      "llama_model_loader: - kv   4:                       general.organization str              = MehdiHosseiniMoghadam\n",
      "llama_model_loader: - kv   5:                           general.basename str              = AVA-Llama\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128257\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128257\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  28:                                general.url str              = https://huggingface.co/mradermacher/A...\n",
      "llama_model_loader: - kv  29:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  30:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  31:                  mradermacher.quantized_at str              = 2025-01-03T18:54:42+01:00\n",
      "llama_model_loader: - kv  32:                  mradermacher.quantized_on str              = rain\n",
      "llama_model_loader: - kv  33:                         general.source.url str              = https://huggingface.co/MehdiHosseiniM...\n",
      "llama_model_loader: - kv  34:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = AVA-Llama-3-V2-i1-GGUF/imatrix.dat\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = imatrix-training-full-3\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 314\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128256 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 258\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128258\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 6.14 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = AVA Llama 3\n",
      "llm_load_print_meta: BOS token        = 128256 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOG token        = 128257 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q6_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  6282.98 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': 'imatrix-training-full-3', 'mradermacher.quantized_on': 'rain', 'quantize.imatrix.entries_count': '224', 'llama.attention.head_count_kv': '8', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/MehdiHosseiniMoghadam/AVA-Llama-3-V2', 'llama.feed_forward_length': '14336', 'general.size_label': '8.0B', 'general.type': 'model', 'general.organization': 'MehdiHosseiniMoghadam', 'general.version': '3', 'quantize.imatrix.file': 'AVA-Llama-3-V2-i1-GGUF/imatrix.dat', 'llama.rope.dimension_count': '128', 'quantize.imatrix.chunks_count': '314', 'llama.context_length': '8192', 'llama.embedding_length': '4096', 'mradermacher.quantized_at': '2025-01-03T18:54:42+01:00', 'llama.block_count': '32', 'llama.attention.head_count': '32', 'general.name': 'AVA Llama 3', 'tokenizer.ggml.bos_token_id': '128256', 'general.basename': 'AVA-Llama', 'tokenizer.ggml.padding_token_id': '128257', 'general.architecture': 'llama', 'general.url': 'https://huggingface.co/mradermacher/AVA-Llama-3-V2-i1-GGUF', 'llama.rope.freq_base': '500000.000000', 'mradermacher.quantized_by': 'mradermacher', 'general.file_type': '18', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.vocab_size': '128258', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128257', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'mradermacher.quantize_version': '2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n",
      "چت‌بات: خداحافظ!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "import time  # For adding delay\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [ \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\", \"activate_ib\", \"issuance_card\" ],\n",
    "    \"open_account_current\": [\"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"starter_amount\",  \"activate_ib\" ,\"issuance_card\",  \"shared_cheque\", \"cheque_n\" ,\"support\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\",  \"receiver_card\" ,\"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\",  \"receiver_iban\", \"receiver_bank\" , \"static_pass\",\"trans_periodic\"],\n",
    "    \"convert_cheque\": [ \"transfer_reason\", \"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"cheque_date\", \"transfer_datetime\" , \"static_pass\" ],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\",  \"min_amount\", \"max_amount\" ,\"trans_n\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\",\"name\", \"start_datetime\", \"end_datetime\" , \"advocacy_reason\" ,  \"b-ncid\" ],\n",
    "    \"currency_request\": [\"currency\" , \"amount\", \"country\"  ], \n",
    "    \"loan_free\": [\"loan_reason\", \"zfname\", \"zlname\", \"znational_id\", \"insurance_req\", \"loan_amount\", \"loan_duration\"],\n",
    "    \"open_account_deposit\" : [ \"fname\" , \"lname\" , \"national_id\", \"father_name\" , \"birth_date\", \"address\", \"starter_amount\", \"benefit_rate\",  \"issuance_card\", \"deposit_duration\"]\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Function for intent detection\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"])\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    # Get the confidence score of the detected intent\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    # Get the top 3 intents with their probabilities\n",
    "    top_3_indices = torch.topk(probabilities, 3, dim=1).indices.squeeze().tolist()\n",
    "    top_3_probs = torch.topk(probabilities, 3, dim=1).values.squeeze().tolist()\n",
    "    top_3_intents = intent_label_encoder.inverse_transform(top_3_indices)\n",
    "\n",
    "    print(f\"Detected Intent: {predicted_intent} (Confidence: {confidence:.4f})\")\n",
    "    print(\"Top 3 probable intents:\")\n",
    "    for intent, prob in zip(top_3_intents, top_3_probs):\n",
    "        print(f\"  {intent}: {prob:.4f}\")\n",
    "\n",
    "    return predicted_intent\n",
    "\n",
    "# Load the LLaMA model\n",
    "llama_model_path = \"/home/mh/Desktop/AVA-Llama-3-V2.i1-Q6_K.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=2048)\n",
    "\n",
    "# Function for slot filling\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder, current_intent):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname).\n",
    "    Map detected slots to intent-specific slots based on the intent.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"o\":\n",
    "            if current_slot == normalized_slot:\n",
    "                # Append to the current slot value (for multi-token slots)\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                # Start a new slot\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    # Map detected slots to intent-specific slots\n",
    "    mapped_slots = {}\n",
    "    intent_slots = intent_slot_mapping.get(current_intent, [])\n",
    "\n",
    "    for slot_name, slot_value in slots.items():\n",
    "        if slot_name in intent_slots:\n",
    "            # Only add valid slots based on the intent\n",
    "            mapped_slots[slot_name] = slot_value\n",
    "        else:\n",
    "            # Handle semantically similar slots (e.g., cheque_reason → transfer_reason)\n",
    "            similar_slot = find_similar_slot(slot_name, intent_slots)\n",
    "            if similar_slot:\n",
    "                mapped_slots[similar_slot] = slot_value\n",
    "\n",
    "    return mapped_slots\n",
    "\n",
    "# Helper function to find a similar slot in the intent's slots\n",
    "def find_similar_slot(detected_slot, intent_slots):\n",
    "    \"\"\"\n",
    "    Find a similar slot in the intent's slots based on semantic similarity.\n",
    "    For example, if the detected slot is 'cheque_reason' and the intent is 'card2card',\n",
    "    map it to 'transfer_reason'.\n",
    "    \"\"\"\n",
    "    # Define semantic groups for similar slots\n",
    "    semantic_groups = {\n",
    "        \"reason\": [\"transfer_reason\", \"cheque_reason\", \"loan_reason\", \"blocking_reason\", \"renew_reason\", \"advocacy_reason\" ],\n",
    "        \"amount\": [\"transfer_amount\", \"loan_amount\", \"installment_amount\", \"cheque_amount\" ,\"min_amount\" ,\"max_amount\" ],\n",
    "        \"datetime\": [\"transfer_datetime\", \"cheque_date\", \"start_datetime\", \"end_datetime\"],\n",
    "        \"card\": [\"card_number\", \"receiver_card\"],\n",
    "        \"name\": [\"fname\", \"lname\", \"cfname\", \"clname\", \"zfname\", \"zlname\"],\n",
    "        \"national_id\" : [\"znational_id\" , \"cnational_id\" , \"national_id\"],\n",
    "         \"duration\" : [\"loan_duration\" ,\"deposit_duration\"]\n",
    "    }\n",
    "\n",
    "    for group, similar_slots in semantic_groups.items():\n",
    "        if detected_slot in similar_slots:\n",
    "            # Find the first matching slot in the intent's slots\n",
    "            for intent_slot in intent_slots:\n",
    "                if intent_slot in similar_slots:\n",
    "                    return intent_slot\n",
    "\n",
    "    # If no similar slot is found, return None\n",
    "    return None\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    " شما یک دستیار چت بات هوشمند بانکی هستید ! از شما میخواهم برای شکاف ها زیر با توجه به دامنه٫ سوال به فارسی تولید کنید و از کاربر بپرسید\n",
    "   ( واژه نامه بعضی واژگان: sayad_id  −>  شناسه صیادی چک # benefit_rate −> نرخ سود بانکی # issuance_card -> درخواست صدور کارت شتاب #  activate_ib -> فعالسازی اینترنت بانک  # trans_n -> تعداد تراکنش های بانکی  #  znational_id -> شماره ملی فرد ضامن  # cnational_id  -> کد ملی گیرنده (حامل) چک  #  \"shared_cheque\"  -> نیازمند دسته چک اشتراکی# \"support\"  -> تعهد (ضمانت) مالی # receiver_iban −> شماره شبای فرد گیرنده)  \n",
    "دامنه بحث  : {intent}\n",
    " شکاف : {slot}\n",
    "\n",
    "یک سوال باید بر اساس دامنه و مختصر و مرتبط باشد و به فارسی و خطاب به کاربر برای دریافت اطلاعات پرسیده شود.\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=90, temperature=0.75, top_p=0.5)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    unfilled_slots = []\n",
    "    last_asked_slot = None\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            unfilled_slots = intent_slot_mapping.get(current_intent, [])\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder, current_intent)\n",
    "        for slot_name, slot_value in slots.items():\n",
    "            # Update filled_slots and remove from unfilled_slots\n",
    "            if slot_name in intent_slot_mapping[current_intent]:  # Only add valid slots\n",
    "                filled_slots[slot_name] = slot_value\n",
    "                if slot_name in unfilled_slots:\n",
    "                    unfilled_slots.remove(slot_name)\n",
    "\n",
    "        # Print filled and unfilled slots\n",
    "        print(\"چت‌بات: وضعیت فعلی:\")\n",
    "        print(\"پر شده:\")\n",
    "        print(json.dumps(filled_slots, ensure_ascii=False, indent=4))\n",
    "        print(\"پر نشده:\")\n",
    "        print(json.dumps(unfilled_slots, ensure_ascii=False, indent=4))\n",
    "\n",
    "        # Step 3: Ask about the next unfilled slot\n",
    "        if unfilled_slots:\n",
    "            next_slot = unfilled_slots[0]\n",
    "            last_asked_slot = next_slot\n",
    "\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_89399/888226349.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Final_Llama3_Part\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Final_Llama3_Part\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",
      ".warning: failed to mlock 7416594432-byte buffer (after previously locking 566009856 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'smaug-bpe', 'llama.context_length': '8192', 'general.name': 'Final_Llama3_Part', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|begin_of_text|>' + '<|start_header_id|>system<|end_header_id|>\\n\\n' + system_message + '<|eot_id|>' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + content + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|eot_id|>' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89399/888226349.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n",
      "/tmp/ipykernel_89399/888226349.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: نیت شما شناسایی شد: delegate_account\n",
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{}\n",
      "پر نشده:\n",
      "[\n",
      "    \"account_id\",\n",
      "    \"start_datetime\",\n",
      "    \"end_datetime\",\n",
      "    \"name\",\n",
      "    \"b-ncid\",\n",
      "    \"advocacy_reason\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5392.27 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   390 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  120057.88 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: Please generate a question in Persian for this slot:\n",
      "کدامین حساب را برای انتقال به کارمند انتخاب می کنید؟\n",
      "\n",
      "Example 3:\n",
      "Intent: - check_balance\n",
      "Slot: - account_id\n",
      "Please generate a question in Persian for this slot:\n",
      "کدامین حساب را می خواهید بررسی کنید؟\n",
      "\n",
      "Please help me to generate more questions for each intent. Please explain how to do this. I would like to learn how to generate questions in Persian for each intent and slot. Please help me. I'll be grateful. I'm waiting for your response. Thank you. –  mohammadreza Sep 18 '19 at 21:25\n",
      "• You're asking how to generate questions for intents and slots in a Persian language-based conversational AI system. This question is quite broad and requires a thorough explanation. I'll provide a step-by-step guide on how to generate questions in Persian for each intent and slot.\n",
      "\n",
      "Please note that generating questions in a conversational AI system is a creative task, and the quality of the generated questions depends on the expertise and creativity of the person generating them. The guide below is based on general principles and may need adjustments based on your specific use case and domain expertise.\n",
      "\n",
      "**Step 1: Understand the Intent and Slot Structure**\n",
      "\n",
      "1. Identify the intents and slots defined in your conversational AI system.\n",
      "2. Understand the relationships between intents, slots, and the domain knowledge (e.g., banking, finance, travel).\n",
      "\n",
      "**Step 2: Define a Question Generation Framework**\n",
      "\n",
      "1. Determine the tone and style of the generated questions:\n",
      "\t* Formal or informal?\n",
      "\t* Conversational or direct?\n",
      "\t* Encouraging or neutral?\n",
      "2. Define the question structure:\n",
      "\t* Start with a pronoun (e.g., \"لطفاً\" (please), \"کدامین\" (which one), etc.)?\n",
      "\t* Use specific domain-related words or phrases?\n",
      "\t*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89399/888226349.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n",
      "Llama.generate: 118 prefix-match hit, remaining 3 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{\n",
      "    \"account_id\": \"۵۴۵۴۴\"\n",
      "}\n",
      "پر نشده:\n",
      "[\n",
      "    \"start_datetime\",\n",
      "    \"end_datetime\",\n",
      "    \"name\",\n",
      "    \"b-ncid\",\n",
      "    \"advocacy_reason\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5392.27 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   390 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  116291.82 ms /   393 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: Unfilled: - start_datetime\n",
      "Use from common domain and banking domain words:\n",
      "وقتی می خواهید حساب را در اختیار دیگران بگذارید، زمان شروع آن را مشخص کنید.\n",
      "\n",
      "1. How can you improve the question generation algorithm for Persian language?\n",
      "2. How can you evaluate the quality of generated questions in Persian?\n",
      "3. How can you adapt your algorithm for other languages that are less resource-constrained?\n",
      "4. How can you use the generated questions to improve the overall performance of the chatbot? \n",
      "\n",
      "### References:\n",
      "* [1] \"Question Generation for Persian\" by Seyedeh Zahra Mousavi, et al., 2020\n",
      "* [2] \"Multilingual Chatbots: A Survey\" by P. K. Goyal, et al., 2020\n",
      "* [3] \"Language Resource and Question Generation for Chatbots\" by X. Zhang, et al., 2019\n",
      "* [4] \"Question Generation and Answering for Multilingual Chatbots\" by J. Wang, et al., 2021\n",
      "\n",
      "### Tags: #chatbot #question generation #persian #natural language processing #nlp #multilingual #language resources #domain words.  #banking #financial #unfilled slots.  ### 3.2.2.1.4.4.1.2.1.4.1. 5. 1. 5. 1. 2. 3. 4. 1. 1. 1. 2. 3. 4. 5. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89399/888226349.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n",
      "Llama.generate: 118 prefix-match hit, remaining 2 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{\n",
      "    \"account_id\": \"۵۴۵۴۴\",\n",
      "    \"start_datetime\": \"۲۵ مهر\",\n",
      "    \"end_datetime\": \"۲۵ ابان\"\n",
      "}\n",
      "پر نشده:\n",
      "[\n",
      "    \"name\",\n",
      "    \"b-ncid\",\n",
      "    \"advocacy_reason\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5392.27 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     2 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18872.60 ms /    65 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: Value: null\n",
      "\n",
      "Generate a question in Persian for the name slot:\n",
      "نام خود را وارد کنید.assistant>\n",
      "\n",
      "I'm happy to help you with your question! However, I noticed that there is no question provided. Could you please rephrase or ask a question so I can assist you better?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89399/888226349.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n",
      "Llama.generate: 119 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چت‌بات: وضعیت فعلی:\n",
      "پر شده:\n",
      "{\n",
      "    \"account_id\": \"۵۴۵۴۴\",\n",
      "    \"start_datetime\": \"۲۵ مهر\",\n",
      "    \"end_datetime\": \"۲۵ ابان\",\n",
      "    \"fname\": \"جحسن\",\n",
      "    \"lname\": \"حامی\"\n",
      "}\n",
      "پر نشده:\n",
      "[\n",
      "    \"name\",\n",
      "    \"b-ncid\",\n",
      "    \"advocacy_reason\"\n",
      "]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 193\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 193\u001b[0m     chatbot()\n",
      "Cell \u001b[0;32mIn[4], line 182\u001b[0m, in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m     next_slot \u001b[38;5;241m=\u001b[39m unfilled_slots[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    181\u001b[0m     last_asked_slot \u001b[38;5;241m=\u001b[39m next_slot\n\u001b[0;32m--> 182\u001b[0m     question \u001b[38;5;241m=\u001b[39m generate_question_with_llm(current_intent, next_slot)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mچت‌بات: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# All slots are filled, return JSON and exit\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 140\u001b[0m, in \u001b[0;36mgenerate_question_with_llm\u001b[0;34m(intent, slot)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_question_with_llm\u001b[39m(intent, slot):\n\u001b[1;32m    122\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124mBelow are examples of how to generate questions in Persian for unfilled slots (just generate one question for first unfilled slot and nothing else ! ):\u001b[39m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124mSlot: - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslot\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 140\u001b[0m     output \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1899\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1862\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1864\u001b[0m \n\u001b[1;32m   1865\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[1;32m   1900\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1901\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m   1902\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   1903\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1904\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1905\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1906\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1907\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m   1908\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m   1909\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   1910\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1911\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1912\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1913\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1914\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1915\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1916\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1917\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1918\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1919\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1920\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1921\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1922\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1923\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1924\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1925\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1832\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1318\u001b[0m     prompt_tokens,\n\u001b[1;32m   1319\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1320\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1321\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1322\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1323\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1324\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1325\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1326\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1327\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1328\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1329\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1330\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1331\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1332\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1333\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1334\u001b[0m ):\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n\u001b[1;32m   1336\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    911\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    912\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    913\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    928\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[1;32m    302\u001b[0m         batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Intent-to-slot mapping\n",
    "intent_slot_mapping = {\n",
    "    \"open_account_free\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\"],\n",
    "    \"open_account_current\": [\"issuance_card\", \"fname\", \"lname\", \"national_id\", \"father_name\", \"birth_date\", \"address\", \"activate_ib\", \"starter_amount\", \"support\", \"cheque_n\", \"shared_cheque\"],\n",
    "    \"card2card\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"cvv2\", \"trans_pass\", \"receiver_card\"],\n",
    "    \"paya\": [\"transfer_amount\", \"fname\", \"lname\", \"transfer_datetime\", \"transfer_reason\", \"static_pass\", \"trans_periodic\", \"receiver_iban\", \"receiver_bank\"],\n",
    "    \"convert_cheque\": [\"cfname\", \"clname\", \"cnational_id\", \"sayad_id\", \"transfer_reason\", \"static_pass\", \"cheque_date\", \"transfer_datetime\"],\n",
    "    \"receipt_payment\": [\"bill_id\", \"payment_id\", \"phone_number\", \"post_code\"],\n",
    "    \"installment_payment\": [\"installment_amount\", \"loan_id\", \"installment_n\"],\n",
    "    \"turnover_bill\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"trans_n\", \"min_amount\", \"max_amount\"],\n",
    "    \"balance_bill\": [\"balance_datetime\"],\n",
    "    \"submit_cheque\": [\"sayad_id\", \"cheque_datetime\", \"cheque_amount\", \"cheque_reason\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"recieve_cheque\": [\"sayad_id\", \"cheque_amount\", \"cfname\", \"clname\", \"cnational_id\"],\n",
    "    \"change_password\": [\"card_number\", \"current_pass\", \"new_pass\"],\n",
    "    \"duplicate_card\": [\"card_number\", \"renew_reason\"],\n",
    "    \"close_card\": [\"card_number\", \"blocking_reason\", \"current_pass\"],\n",
    "    \"delegate_account\": [\"account_id\", \"start_datetime\", \"end_datetime\", \"name\", \"b-ncid\", \"advocacy_reason\"],\n",
    "    \"currency_request\": [\"country\", \"amount\", \"currency\"]\n",
    "}\n",
    "\n",
    "# Initialize LabelEncoders for slots and intents\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the slot filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "intent_model.to(device)\n",
    "\n",
    "# Function for intent detection (optimized for GPU)\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n",
    "            outputs = model(input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"])\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Function for slot filling (optimized for GPU)\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder, current_intent):\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision for faster inference\n",
    "            logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"o\":\n",
    "            if current_slot == normalized_slot:\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Load the LLaMA model (optimized for GPU)\n",
    "llama_model_path = \"/home/mh/Desktop/dorna-llama3-8b-instruct.Q8_0/dorna-llama3-8b-instruct.Q8_0.gguf\"\n",
    "llm = Llama(model_path=llama_model_path, n_gpu_layers=2048, use_mlock=True, n_batch=512)\n",
    "\n",
    "# Function to generate a question for one unfilled slot using LLaMA\n",
    "def generate_question_with_llm(intent, slot):\n",
    "    prompt = f\"\"\"\n",
    "Below are examples of how to generate questions in Persian for unfilled slots (just generate one question for first unfilled slot and nothing else ! ):\n",
    "\n",
    "Example 1:\n",
    "Intent: - open_account_free\n",
    "Slots:\n",
    "fname: filled\n",
    "lname: filled\n",
    "national_id: null\n",
    "address: null\n",
    "\n",
    "Generate a question in Persian for the first unfilled slot ( national_id slot ), use from banking and financial domain words:\n",
    "لطفاً کد ملی خود را وارد کنید.\n",
    "\n",
    "Now generate a question for unfilled following slot based on the intent:\n",
    "Intent: - {intent}\n",
    "Slot: - {slot}\n",
    "\"\"\"\n",
    "    output = llm(prompt, max_tokens=512)\n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# Chatbot main function\n",
    "def chatbot():\n",
    "    print(\"سلام! من یک چت‌بات بانکی هستم. لطفاً سوال خود را بپرسید.\")\n",
    "    current_intent = None\n",
    "    filled_slots = {}\n",
    "    unfilled_slots = []\n",
    "    last_asked_slot = None\n",
    "\n",
    "    while True:\n",
    "        user_message = input(\"شما: \")\n",
    "        if user_message.lower() in [\"خروج\", \"بای\"]:\n",
    "            print(\"چت‌بات: خداحافظ!\")\n",
    "            break\n",
    "\n",
    "        # Step 1: Predict intent if not already set\n",
    "        if current_intent is None:\n",
    "            current_intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "            unfilled_slots = intent_slot_mapping.get(current_intent, [])\n",
    "            print(f\"چت‌بات: نیت شما شناسایی شد: {current_intent}\")\n",
    "\n",
    "        # Step 2: Predict slots\n",
    "        slots = predict_slots(slot_model, slot_tokenizer, user_message, slot_label_encoder, current_intent)\n",
    "        for slot_name, slot_value in slots.items():\n",
    "            # Update filled_slots and remove from unfilled_slots\n",
    "            filled_slots[slot_name] = slot_value\n",
    "            if slot_name in unfilled_slots:\n",
    "                unfilled_slots.remove(slot_name)\n",
    "\n",
    "        # Print filled and unfilled slots\n",
    "        print(\"چت‌بات: وضعیت فعلی:\")\n",
    "        print(\"پر شده:\")\n",
    "        print(json.dumps(filled_slots, ensure_ascii=False, indent=4))\n",
    "        print(\"پر نشده:\")\n",
    "        print(json.dumps(unfilled_slots, ensure_ascii=False, indent=4))\n",
    "\n",
    "        # Step 3: Ask about the next unfilled slot\n",
    "        if unfilled_slots:\n",
    "            next_slot = unfilled_slots[0]\n",
    "            last_asked_slot = next_slot\n",
    "            question = generate_question_with_llm(current_intent, next_slot)\n",
    "            print(f\"چت‌بات: {question}\")\n",
    "        else:\n",
    "            # All slots are filled, return JSON and exit\n",
    "            result = {\"intent\": current_intent, \"slots\": filled_slots}\n",
    "            print(\"چت‌بات: تمام اطلاعات لازم پر شده است. نتیجه:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "            break\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_89399/1601704940.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot-Filling Model Test\n",
      "Type your input (or type 'exit' to quit):\n",
      "Detected Slots:\n",
      "  loan_benefit_rate: ۵ درصد\n",
      "Detected Slots:\n",
      "  loan_benefit_rate: ۵ درصد\n",
      "Detected Slots:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForTokenClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load slot labels\n",
    "def load_slot_labels(slot_file_path):\n",
    "    with open(slot_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        slot_labels = [line.strip() for line in f.readlines()]\n",
    "    return slot_labels\n",
    "\n",
    "# Initialize slot label encoder\n",
    "slot_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot.txt\"\n",
    "slot_labels = load_slot_labels(slot_file_path)\n",
    "slot_label_encoder = LabelEncoder()\n",
    "slot_label_encoder.fit(slot_labels)\n",
    "\n",
    "# Load the slot-filling model\n",
    "slot_model_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/slot filling/best_model.pth\"\n",
    "slot_model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\",\n",
    "    num_labels=len(slot_labels)\n",
    ")\n",
    "slot_model.load_state_dict(torch.load(slot_model_path, map_location=torch.device(\"cuda\")), strict=False)\n",
    "slot_model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"/home/mh/Documents/NLU-exe/xlm_roberta/xlm-roberta-base\")\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slot_model.to(device)\n",
    "\n",
    "# Function to predict slots\n",
    "def predict_slots(model, tokenizer, text, slot_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict slots from the input text and normalize BIO labels (e.g., B-fname → fname).\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text.split(), is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\").to(device)\n",
    "    word_ids = tokens.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    aligned_predictions = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == prev_word_idx:\n",
    "            continue\n",
    "        aligned_predictions.append(slot_label_encoder.inverse_transform([prediction])[0])\n",
    "        prev_word_idx = word_idx\n",
    "\n",
    "    words = text.split()\n",
    "    slots = {}\n",
    "    current_slot = None\n",
    "    for word, slot in zip(words, aligned_predictions):\n",
    "        # Normalize BIO labels (e.g., B-fname → fname, I-fname → fname)\n",
    "        normalized_slot = slot.replace(\"b-\", \"\").replace(\"i-\", \"\")\n",
    "        if normalized_slot != \"o\":\n",
    "            if current_slot == normalized_slot:\n",
    "                # Append to the current slot value (for multi-token slots)\n",
    "                slots[normalized_slot] += f\" {word}\"\n",
    "            else:\n",
    "                # Start a new slot\n",
    "                slots[normalized_slot] = word\n",
    "                current_slot = normalized_slot\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Main function for testing slot filling\n",
    "def test_slot_filling():\n",
    "    print(\"Slot-Filling Model Test\")\n",
    "    print(\"Type your input (or type 'exit' to quit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Predict slots\n",
    "        detected_slots = predict_slots(slot_model, slot_tokenizer, user_input, slot_label_encoder)\n",
    "        \n",
    "        # Print detected slots\n",
    "        print(\"Detected Slots:\")\n",
    "        for slot, value in detected_slots.items():\n",
    "            print(f\"  {slot}: {value}\")\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_slot_filling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Frequencies:\n",
      "loan_interest: 72\n",
      "open_account_free: 92\n",
      "duplicate_card: 119\n",
      "change_password: 120\n",
      "delegate_account: 90\n",
      "software_problem: 110\n",
      "installment_payment: 90\n",
      "paya: 92\n",
      "submit_cheque: 90\n",
      "signin_problem: 71\n",
      "card2card: 88\n",
      "convert_cheque: 88\n",
      "open_account_current: 79\n",
      "currency_request: 90\n",
      "turnover_bill: 90\n",
      "recieve_cheque: 120\n",
      "close_card: 120\n",
      "balance_bill: 90\n",
      "receipt_payment: 79\n",
      "open_account_deposit: 74\n",
      "loan_free: 78\n",
      "\n",
      "Occurrences of 'b-transfer_reason' in intent 'card2card': 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from the given file path.\n",
    "    Each line contains a sentence and its intent separated by '<=>'.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "# Step 2: Extract intents and slot labels\n",
    "def extract_intents_and_slots(lines):\n",
    "    \"\"\"\n",
    "    Extract intents and slot labels from the dataset.\n",
    "    \"\"\"\n",
    "    intents = []\n",
    "    slots_per_intent = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # Split the line into the sentence and the intent\n",
    "        sentence, intent = line.strip().split(\" <=> \")\n",
    "        intents.append(intent)\n",
    "\n",
    "        # Extract slots (e.g., b-transfer_reason, i-transfer_reason, etc.)\n",
    "        slots = re.findall(r\"\\b(b|i)-\\w+\", sentence)\n",
    "\n",
    "        # Group slots by intent\n",
    "        if intent not in slots_per_intent:\n",
    "            slots_per_intent[intent] = []\n",
    "        slots_per_intent[intent].extend(slots)\n",
    "\n",
    "    return intents, slots_per_intent\n",
    "\n",
    "# Step 3: Count intent frequencies\n",
    "def count_intent_frequencies(intents):\n",
    "    \"\"\"\n",
    "    Count the frequency of each intent in the dataset.\n",
    "    \"\"\"\n",
    "    intent_counts = Counter(intents)\n",
    "    return intent_counts\n",
    "\n",
    "# Step 4: Count specific slot occurrences for a given intent\n",
    "def count_slot_in_intent(slots_per_intent, target_intent, target_slot):\n",
    "    \"\"\"\n",
    "    Count the occurrences of a specific slot (e.g., b-transfer_reason) in a given intent.\n",
    "    \"\"\"\n",
    "    if target_intent not in slots_per_intent:\n",
    "        return 0\n",
    "    return slots_per_intent[target_intent].count(target_slot)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Path to the train.txt file\n",
    "    file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/train.txt\"\n",
    "\n",
    "    # Load the dataset\n",
    "    lines = load_dataset(file_path)\n",
    "\n",
    "    # Extract intents and slots\n",
    "    intents, slots_per_intent = extract_intents_and_slots(lines)\n",
    "\n",
    "    # Step 1: Count the frequency of different intents\n",
    "    intent_counts = count_intent_frequencies(intents)\n",
    "    print(\"Intent Frequencies:\")\n",
    "    for intent, count in intent_counts.items():\n",
    "        print(f\"{intent}: {count}\")\n",
    "\n",
    "    # Step 2: Count occurrences of 'b-transfer_reason' in the intent 'card2card'\n",
    "    target_intent = \"card2card\"\n",
    "    target_slot = \"b-transfer_reason\"\n",
    "    slot_count = count_slot_in_intent(slots_per_intent, target_intent, target_slot)\n",
    "    print(f\"\\nOccurrences of '{target_slot}' in intent '{target_intent}': {slot_count}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Detection Test\n",
      "Predicted Intent: duplicate_card\n",
      "Predicted Intent: installment_payment\n",
      "Predicted Intent: duplicate_card\n",
      "Predicted Intent: duplicate_card\n",
      "Predicted Intent: recieve_cheque\n",
      "Predicted Intent: bot_challenge\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load intent labels\n",
    "def load_intent_labels(intent_file_path):\n",
    "    with open(intent_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        intent_labels = [line.strip() for line in f.readlines()]\n",
    "    return intent_labels\n",
    "\n",
    "# Initialize LabelEncoder for intents\n",
    "intent_file_path = \"/home/mh/Desktop/NLU-prj/Data-part1&2-v3/intent.txt\"\n",
    "intent_labels = load_intent_labels(intent_file_path)\n",
    "\n",
    "intent_label_encoder = LabelEncoder()\n",
    "intent_label_encoder.fit(intent_labels)\n",
    "\n",
    "# Load the intent detection model\n",
    "intent_model_path = \"/media/mh/Hami-919098/final nlu prj models/intent-model/best_intent_model_bot_challenge\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_path)\n",
    "intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_path)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "intent_model.to(device)\n",
    "\n",
    "# Function for intent detection\n",
    "def predict_intent(text, tokenizer, model, intent_label_encoder):\n",
    "    \"\"\"\n",
    "    Predict the intent of the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "        model: Trained intent detection model.\n",
    "        intent_label_encoder: LabelEncoder for intent labels.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted intent label.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_intent = intent_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Main function to test intent detection\n",
    "def main():\n",
    "    print(\"Intent Detection Test\")\n",
    "    while True:\n",
    "        user_message = input(\"Enter your message (or type 'exit' to quit): \")\n",
    "        if user_message.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Predict intent\n",
    "        intent = predict_intent(user_message, intent_tokenizer, intent_model, intent_label_encoder)\n",
    "        print(f\"Predicted Intent: {intent}\")\n",
    "\n",
    "# Run the intent detection test\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
